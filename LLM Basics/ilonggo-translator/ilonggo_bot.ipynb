{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5d6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\User\\Desktop\\Acads\\4th Year\\TwoTabs Dir\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Import all required libraries\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # ‚úÖ Local embeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5433138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini API key loaded\n",
      "‚úÖ Groq API key loaded\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load API keys from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verify keys are loaded\n",
    "gemini_key = os.getenv('GEMINI_API_KEY')\n",
    "groq_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if gemini_key:\n",
    "    print(\"‚úÖ Gemini API key loaded\")\n",
    "if groq_key:\n",
    "    print(\"‚úÖ Groq API key loaded\")\n",
    "    \n",
    "if not gemini_key and not groq_key:\n",
    "    print(\"‚ö†Ô∏è No API keys found! Create .env file with your keys\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c2711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Gemini Flash 2.0 for chat\n",
      "   - Free tier: 1,500 requests/day\n",
      "\n",
      "üì• Loading local embedding model...\n",
      "   (First time takes ~30 seconds to download model)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Client\\AppData\\Local\\Temp\\ipykernel_21648\\1058934120.py:30: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632aecebc1614343a5b8a1aa12b8aca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\User\\Desktop\\Acads\\4th Year\\TwoTabs Dir\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Client\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe32235cdc246c68d0a5af48b271ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5851158325844224bbe9bdb7ce439531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4ec6de9482416995d6e6b1729989a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21db416a647a49b48d5a3c3e01bfa490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308900e18a7342f7be03aece9c54d959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e828f4c3ff4952821fe50739b7ccb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b01489641b4205b4ca6285bab6e77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585383f5046c44c6a6eb8270d22cc2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d6e79a98549a6b004d2d244c903f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484d7febe7814a7ebabf6cd27ffb070f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d1b4feb5c04c669cba283c1e010fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ LOCAL EMBEDDINGS configured!\n",
      "   - Model: all-MiniLM-L6-v2\n",
      "   - Size: ~90MB (one-time download)\n",
      "   - Speed: Fast on CPU\n",
      "   - Rate limits: UNLIMITED! üéâ\n",
      "\n",
      "‚úÖ Complete setup: gemini + Local Embeddings\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Configure LLM and LOCAL embeddings (no rate limits!)\n",
    "\n",
    "# Choose your chat model\n",
    "MODEL_CHOICE = \"gemini\"  # Change to \"groq\" if you prefer\n",
    "\n",
    "# Configure chat model\n",
    "if MODEL_CHOICE == \"gemini\":\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        google_api_key=os.getenv('GEMINI_API_KEY'),\n",
    "        temperature=0.3,  # Lower = more precise translations\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    print(\"üöÄ Using Gemini Flash 2.0 for chat\")\n",
    "    print(\"   - Free tier: 1,500 requests/day\")\n",
    "\n",
    "elif MODEL_CHOICE == \"groq\":\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "        temperature=0.3\n",
    "    )\n",
    "    print(\"üöÄ Using Groq Llama 3.1 8B for chat\")\n",
    "    print(\"   - Free tier: 14,400 requests/day\")\n",
    "\n",
    "# ‚úÖ LOCAL EMBEDDINGS (unlimited, no API calls!)\n",
    "print(\"\\nüì• Loading local embedding model...\")\n",
    "print(\"   (First time takes ~30 seconds to download model)\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ LOCAL EMBEDDINGS configured!\")\n",
    "print(\"   - Model: all-MiniLM-L6-v2\")\n",
    "print(\"   - Size: ~90MB (one-time download)\")\n",
    "print(\"   - Speed: Fast on CPU\")\n",
    "print(\"   - Rate limits: UNLIMITED! üéâ\")\n",
    "print(f\"\\n‚úÖ Complete setup: {MODEL_CHOICE} + Local Embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89335096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading Ilonggo dictionaries from 'dictionaries/' folder...\n",
      "   (Any PDF filename works - all PDFs will be loaded)\n",
      "\n",
      "‚úÖ Loaded 596 pages from PDF dictionaries\n",
      "\n",
      "üìñ Sample text from first page:\n",
      "English ‚Äì Hiligaynon (Ilongo)\n",
      "a ( indefinite article) isa \n",
      "aback ( to be taken aback) palak \n",
      "abandon pabayaan , abandonar \n",
      "abandoned sim-ong \n",
      "abatoir ihawan \n",
      "abbreviation lip-ot \n",
      "ABC abakada \n",
      "abdomen ...\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Load all PDF dictionaries from 'dictionaries/' folder\n",
    "# ‚ö†Ô∏è IMPORTANT: Put your PDF files in 'dictionaries/' folder first!\n",
    "\n",
    "print(\"üìö Loading Ilonggo dictionaries from 'dictionaries/' folder...\")\n",
    "print(\"   (Any PDF filename works - all PDFs will be loaded)\")\n",
    "\n",
    "# Load all PDFs from dictionaries folder\n",
    "loader = PyPDFDirectoryLoader(\"./dictionaries/\")\n",
    "\n",
    "try:\n",
    "    documents = loader.load()\n",
    "    print(f\"\\n‚úÖ Loaded {len(documents)} pages from PDF dictionaries\")\n",
    "    \n",
    "    # Show first 200 characters to verify\n",
    "    if documents:\n",
    "        print(f\"\\nüìñ Sample text from first page:\")\n",
    "        print(f\"{documents[0].page_content[:200]}...\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No PDFs found!\")\n",
    "        print(\"   1. Create 'dictionaries/' folder in project root\")\n",
    "        print(\"   2. Add your Ilonggo dictionary PDFs to it\")\n",
    "        print(\"   3. Run this cell again\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading PDFs: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Make sure 'dictionaries/' folder exists\")\n",
    "    print(\"  2. Check PDFs are not password-protected\")\n",
    "    print(\"  3. Verify file paths are correct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3a16d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Splitting dictionary into chunks...\n",
      "\n",
      "‚úÖ Created 9622 searchable chunks\n",
      "   - Average chunk size: ~500 characters\n",
      "   - Overlap: 50 characters\n",
      "\n",
      "üìù Sample chunk preview:\n",
      "English ‚Äì Hiligaynon (Ilongo)\n",
      "a ( indefinite article) isa \n",
      "aback ( to be taken aback) palak \n",
      "abandon pabayaan , abandonar \n",
      "abandoned sim-ong \n",
      "abatoir ...\n",
      "\n",
      "üìä Statistics:\n",
      "   - Total chunks: 9622\n",
      "   - Total characters: 4,501,736\n",
      "   - Average chunk size: 467 chars\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Split dictionary pages into searchable chunks\n",
    "\n",
    "print(\"‚úÇÔ∏è Splitting dictionary into chunks...\")\n",
    "\n",
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Small chunks for dictionary entries\n",
    "    chunk_overlap=50,      # Overlap to avoid cutting words mid-definition\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],  # Split on paragraphs first\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(chunks)} searchable chunks\")\n",
    "print(f\"   - Average chunk size: ~500 characters\")\n",
    "print(f\"   - Overlap: 50 characters\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(f\"\\nüìù Sample chunk preview:\")\n",
    "print(f\"{chunks[0].page_content[:150]}...\")\n",
    "\n",
    "# Show statistics\n",
    "total_chars = sum(len(chunk.page_content) for chunk in chunks)\n",
    "avg_chunk_size = total_chars // len(chunks) if chunks else 0\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   - Total chunks: {len(chunks)}\")\n",
    "print(f\"   - Total characters: {total_chars:,}\")\n",
    "print(f\"   - Average chunk size: {avg_chunk_size} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating searchable vector database with FAISS...\n",
      "‚è≥ This may take 1-3 minutes for large dictionaries...\n",
      "   (Using LOCAL embeddings - no rate limits!)\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Create searchable vector database with FAISS and local embeddings\n",
    "# This creates the RAG index - run this ONCE, then use Cell 6B to reload\n",
    "\n",
    "print(\"üîç Creating searchable vector database with FAISS...\")\n",
    "print(\"‚è≥ This may take 1-3 minutes for large dictionaries...\")\n",
    "print(\"   (Using LOCAL embeddings - no rate limits!)\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create vector store with FAISS\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Save to disk (so you can reload later without reprocessing)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Vector database created with FAISS!\")\n",
    "print(f\"   - Time taken: {elapsed_time:.1f} seconds\")\n",
    "print(f\"   - Stored in: ./faiss_index/\")\n",
    "print(f\"   - Indexed {len(chunks)} dictionary entries\")\n",
    "print(f\"   - Total vectors: {vectorstore.index.ntotal}\")\n",
    "print(\"\\nüí° Next time, use Cell 6B to load instantly from disk!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c78d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6B: Load existing FAISS database from disk\n",
    "# Use this INSTEAD of Cell 6 after you've created the database once\n",
    "# This is much faster than recreating the database!\n",
    "\n",
    "print(\"üîÑ Loading existing FAISS vector database from disk...\")\n",
    "\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        \"faiss_index\", \n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True  # Safe for your own data\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vector database loaded from disk!\")\n",
    "    print(f\"   - Location: ./faiss_index/\")\n",
    "    print(f\"   - Total vectors: {vectorstore.index.ntotal}\")\n",
    "    print(\"   - Ready to use!\")\n",
    "    print(\"\\nüí° This is much faster than re-processing PDFs!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n‚ùå FAISS index not found!\")\n",
    "    print(\"   Run Cell 6 first to create the database.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading database: {e}\")\n",
    "    print(\"   You may need to recreate it with Cell 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Build the translation chain using LangChain Expression Language (LCEL)\n",
    "\n",
    "print(\"üîó Building retrieval chain...\")\n",
    "\n",
    "# Create retriever from vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most relevant dictionary entries\n",
    ")\n",
    "\n",
    "# Custom prompt template for translation\n",
    "template = \"\"\"You are an Ilonggo-English dictionary assistant. Use the dictionary entries below to help translate.\n",
    "\n",
    "Dictionary Context:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- If the word is in the dictionary, provide the definition/translation\n",
    "- If it's English to Ilonggo, search for the English word\n",
    "- If it's Ilonggo to English, search for the Ilonggo word\n",
    "- If not found, say \"I couldn't find that word in the dictionary\"\n",
    "- Be helpful, friendly, and concise\n",
    "- Provide pronunciation help if available in the dictionary\n",
    "\n",
    "Translation:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Combine multiple dictionary entries into context\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the translation chain using LCEL\n",
    "translator_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Ilonggo Translator Chatbot is ready!\")\n",
    "print(\"\\nüí¨ Example questions you can ask:\")\n",
    "print(\"   - 'What does mahal mean?'\")\n",
    "print(\"   - 'How do you say love in Ilonggo?'\")\n",
    "print(\"   - 'Translate pagkaon to English'\")\n",
    "print(\"   - 'What is the Ilonggo word for beautiful?'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Test function to translate and show results\n",
    "\n",
    "def translate(question):\n",
    "    \"\"\"\n",
    "    Translate between Ilonggo and English\n",
    "    Shows the answer and source documents used\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üáµüá≠ YOU: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get translation\n",
    "    answer = translator_chain.invoke(question)\n",
    "    \n",
    "    print(f\"üí¨ TRANSLATOR: {answer}\")\n",
    "    \n",
    "    # Show source documents used\n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"\\nüìö Used {len(docs)} dictionary entries\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# Run test queries\n",
    "test_queries = [\n",
    "    \"What does 'mahal' mean in English?\",\n",
    "    \"How do you say 'hello' in Ilonggo?\",\n",
    "    \"Translate 'kumusta' to English\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Running test translations...\\n\")\n",
    "for query in test_queries:\n",
    "    translate(query)\n",
    "    print()  # Add spacing between tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Interactive chat loop for continuous translation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üáµüá≠ ILONGGO DICTIONARY TRANSLATOR\")\n",
    "print(\"=\"*60)\n",
    "print(\"Ask me to translate between Ilonggo and English!\")\n",
    "print(\"\\nCommands:\")\n",
    "print(\"  - Type your question to translate\")\n",
    "print(\"  - Type 'sources' to see dictionary sources for last query\")\n",
    "print(\"  - Type 'quit' or 'exit' to end\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store last query for source viewing\n",
    "last_query = None\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nüáµüá≠ You: \").strip()\n",
    "    \n",
    "    # Check for exit commands\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"\\nüëã Salamat! (Thank you!)\")\n",
    "        print(\"Goodbye! üáµüá≠\")\n",
    "        break\n",
    "    \n",
    "    # Skip empty input\n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    # Special command to show sources from last query\n",
    "    if user_input.lower() == 'sources':\n",
    "        if last_query:\n",
    "            docs = retriever.invoke(last_query)\n",
    "            print(f\"\\nüìö Dictionary sources for '{last_query}':\")\n",
    "            print(\"=\"*60)\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                print(f\"\\nSource {i}:\")\n",
    "                print(doc.page_content[:300])\n",
    "                print(\"-\"*60)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No previous query. Ask a question first!\")\n",
    "        continue\n",
    "    \n",
    "    # Process translation\n",
    "    try:\n",
    "        # Get translation\n",
    "        answer = translator_chain.invoke(user_input)\n",
    "        print(f\"\\nüí¨ Translator: {answer}\")\n",
    "        \n",
    "        # Store query for potential source viewing\n",
    "        last_query = user_input\n",
    "        \n",
    "        # Ask if user wants to see sources\n",
    "        show_sources = input(\"\\nüìö Show dictionary sources? (y/n): \").strip().lower()\n",
    "        if show_sources == 'y':\n",
    "            docs = retriever.invoke(user_input)\n",
    "            print(f\"\\nüìñ Found {len(docs)} relevant entries:\")\n",
    "            print(\"=\"*60)\n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                print(f\"\\nSource {i}:\")\n",
    "                print(doc.page_content[:300])\n",
    "                if len(doc.page_content) > 300:\n",
    "                    print(\"...\")\n",
    "                print(\"-\"*60)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        print(\"Try rephrasing your question or check your connection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Direct search function for exploring the dictionary\n",
    "\n",
    "def search_dictionary(word, k=5):\n",
    "    \"\"\"\n",
    "    Search the vector database directly for specific words\n",
    "    Returns the k most similar dictionary entries\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(word, k=k)\n",
    "    \n",
    "    print(f\"\\nüîç Searching dictionary for: '{word}'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No matches found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(results)} relevant entries:\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"üìñ Result {i} (Relevance rank: {i}/{k}):\")\n",
    "        print(\"-\"*60)\n",
    "        print(doc.page_content)\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "# search_dictionary(\"mahal\", k=3)\n",
    "\n",
    "print(\"‚úÖ Search function ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  search_dictionary('love', k=3)\")\n",
    "print(\"  search_dictionary('kumusta', k=5)\")\n",
    "print(\"  search_dictionary('maganda', k=2)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Batch translate multiple words at once\n",
    "\n",
    "def batch_translate(words_list):\n",
    "    \"\"\"\n",
    "    Translate multiple words in one go\n",
    "    Useful for learning vocabulary lists\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Batch Translation ({len(words_list)} words)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, word in enumerate(words_list, 1):\n",
    "        print(f\"\\n{i}. Translating '{word}'...\")\n",
    "        try:\n",
    "            answer = translator_chain.invoke(f\"What does {word} mean?\")\n",
    "            results.append({\"word\": word, \"translation\": answer, \"status\": \"success\"})\n",
    "            print(f\"   ‚úÖ {answer[:100]}{'...' if len(answer) > 100 else ''}\")\n",
    "        except Exception as e:\n",
    "            results.append({\"word\": word, \"translation\": str(e), \"status\": \"error\"})\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Batch translation complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "# words_to_translate = [\"mahal\", \"kumusta\", \"salamat\", \"maganda\"]\n",
    "# results = batch_translate(words_to_translate)\n",
    "\n",
    "print(\"‚úÖ Batch translation function ready!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  words = ['mahal', 'kumusta', 'salamat']\")\n",
    "print(\"  results = batch_translate(words)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Complete system test and diagnostics\n",
    "\n",
    "print(\"üß™ Running complete system diagnostics...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Check vectorstore\n",
    "print(\"Test 1: Vector Database\")\n",
    "try:\n",
    "    test_results = vectorstore.similarity_search(\"test\", k=1)\n",
    "    print(f\"‚úÖ Vector database operational\")\n",
    "    print(f\"   - Total vectors: {vectorstore.index.ntotal}\")\n",
    "    print(f\"   - Test query returned: {len(test_results)} results\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vector database error: {e}\")\n",
    "\n",
    "# Test 2: Check retriever\n",
    "print(\"\\nTest 2: Retriever\")\n",
    "try:\n",
    "    test_retrieval = retriever.invoke(\"hello\")\n",
    "    print(f\"‚úÖ Retriever operational\")\n",
    "    print(f\"   - Retrieved: {len(test_retrieval)} documents\")\n",
    "    print(f\"   - Average doc length: {sum(len(d.page_content) for d in test_retrieval) // len(test_retrieval)} chars\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Retriever error: {e}\")\n",
    "\n",
    "# Test 3: Check embeddings\n",
    "print(\"\\nTest 3: Embeddings\")\n",
    "try:\n",
    "    test_embedding = embeddings.embed_query(\"test\")\n",
    "    print(f\"‚úÖ Embeddings operational\")\n",
    "    print(f\"   - Embedding dimensions: {len(test_embedding)}\")\n",
    "    print(f\"   - Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print(f\"   - Rate limits: UNLIMITED (local)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Embeddings error: {e}\")\n",
    "\n",
    "# Test 4: Check LLM\n",
    "print(\"\\nTest 4: Language Model\")\n",
    "try:\n",
    "    test_response = llm.invoke(\"Say hello in one word\")\n",
    "    print(f\"‚úÖ LLM operational\")\n",
    "    print(f\"   - Model: {MODEL_CHOICE}\")\n",
    "    print(f\"   - Test response: {test_response.content[:50]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LLM error: {e}\")\n",
    "\n",
    "# Test 5: Check full translation chain\n",
    "print(\"\\nTest 5: Translation Chain\")\n",
    "try:\n",
    "    test_translation = translator_chain.invoke(\"What does test mean?\")\n",
    "    print(f\"‚úÖ Translation chain operational\")\n",
    "    print(f\"   - Sample response: {test_translation[:80]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Translation chain error: {e}\")\n",
    "\n",
    "# System summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä SYSTEM STATUS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Vector Database: {vectorstore.index.ntotal:,} entries indexed\")\n",
    "print(f\"‚úÖ Embeddings: Local (unlimited usage)\")\n",
    "print(f\"‚úÖ LLM: {MODEL_CHOICE} (API-based)\")\n",
    "print(f\"‚úÖ Total chunks: {len(chunks):,}\")\n",
    "print(\"=\"*60)\n",
    "print(\"üéâ All systems operational! Ready to translate! üáµüá≠\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
