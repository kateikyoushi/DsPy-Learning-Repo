{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bdb4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration ready\n",
      "   Base URL    : https://help.cebupacificair.com\n",
      "   Output file : cebu_pacific_helpcenter.txt\n",
      "   Concurrency : 4 parallel tabs\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 2: Imports & Configuration\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import asyncio, re, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, Page, BrowserContext\n",
    "\n",
    "nest_asyncio.apply()   # Allows asyncio.run() inside Jupyter\n",
    "\n",
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_URL    = \"https://help.cebupacificair.com\"\n",
    "OUTPUT_FILE = \"cebu_pacific_helpcenter.txt\"\n",
    "CONCURRENCY = 4       # Parallel page scrapers (â‰¤5 keeps you polite + hidden)\n",
    "NAV_TIMEOUT = 60_000  # 60s navigation timeout (ms) â€” increased for reliability\n",
    "MIN_DELAY   = 0.5     # Seconds between requests per tab\n",
    "MAX_DELAY   = 1.1\n",
    "\n",
    "# Block heavy resources â€” makes each page load 3-5x faster\n",
    "BLOCK_TYPES = {\"image\", \"media\", \"font\", \"other\"}\n",
    "\n",
    "print(\"âœ… Configuration ready\")\n",
    "print(f\"   Base URL    : {BASE_URL}\")\n",
    "print(f\"   Output file : {OUTPUT_FILE}\")\n",
    "print(f\"   Concurrency : {CONCURRENCY} parallel tabs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056fac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stealth browser helpers ready\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 3: Stealth Browser â€” hides automation signals from the server\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "async def create_stealth_context(playwright):\n",
    "    \"\"\"\n",
    "    Launch headless Chromium with anti-detection patches.\n",
    "    - Removes 'navigator.webdriver' flag\n",
    "    - Sets realistic viewport, UA, locale, timezone\n",
    "    Returns (browser, context).\n",
    "    \"\"\"\n",
    "    browser = await playwright.chromium.launch(\n",
    "        headless=True,\n",
    "        args=[\n",
    "            \"--no-sandbox\",\n",
    "            \"--disable-blink-features=AutomationControlled\",  # Key stealth flag\n",
    "            \"--disable-dev-shm-usage\",\n",
    "            \"--disable-extensions\",\n",
    "            \"--disable-infobars\",\n",
    "        ],\n",
    "    )\n",
    "    context = await browser.new_context(\n",
    "        viewport={\"width\": 1366, \"height\": 768},\n",
    "        user_agent=(\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        locale=\"en-US\",\n",
    "        timezone_id=\"Asia/Manila\",  # Appear as a PH user\n",
    "        extra_http_headers={\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Sec-Fetch-User\":  \"?1\",\n",
    "        },\n",
    "    )\n",
    "    # Patch webdriver property on every page inside this context\n",
    "    await context.add_init_script(\n",
    "        \"Object.defineProperty(navigator, 'webdriver', { get: () => undefined });\"\n",
    "    )\n",
    "    return browser, context\n",
    "\n",
    "\n",
    "async def block_heavy_resources(page: Page):\n",
    "    \"\"\"Abort image/font/media requests â€” keeps page loads fast.\"\"\"\n",
    "    async def _route(route):\n",
    "        if route.request.resource_type in BLOCK_TYPES:\n",
    "            await route.abort()\n",
    "        else:\n",
    "            await route.continue_()\n",
    "    await page.route(\"**/*\", _route)\n",
    "\n",
    "\n",
    "print(\"âœ… Stealth browser helpers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fa6dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¡ Loading homepage...\n",
      "   Found 29 links on homepage\n",
      "Scanning category pages: 1/29 - booking-a-flight\n",
      "Scanning category pages: 2/29 - booking-flights-with-cebu-pacific-121521\n",
      "Scanning category pages: 3/29 - bali-denpasar-airport-and-travel-guide-189038\n",
      "Scanning category pages: 4/29 - cancelations-delays\n",
      "Scanning category pages: 5/29 - ceb-baggage-guidelines-and-upgrade-options-118500\n",
      "Scanning category pages: 6/29 - cebu-pacific-launches-riyadh-flights-212960\n",
      "Scanning category pages: 7/29 - cebu-pacific-domestic-travel-guide-162560\n",
      "Scanning category pages: 8/29 - suspension-of-cebu-pacific-flights-between-cebu-and-222310\n",
      "Scanning category pages: 9/29 - can-i-bring-this-in-my-hand-carry-123016\n",
      "Scanning category pages: 10/29 - prepare-for-your-flight\n",
      "Scanning category pages: 11/29 - can-i-bring-this-in-my-checked-baggage-164532\n",
      "Scanning category pages: 12/29 - cebu-pacific-enters-damp-lease-agreement-with-bulgaria-206011\n",
      "Scanning category pages: 13/29 - bangkok-don-mueang-airport-and-travel-guide-187769\n",
      "Scanning category pages: 14/29 - how-to-contact-cebu-pacific-customer-service-139151\n",
      "Scanning category pages: 15/29 - guangzhou-canton-flights-move-to-baiyun-airport-terminal-220099\n",
      "Scanning category pages: 16/29 - manage-your-booking\n",
      "Scanning category pages: 17/29 - flying-out-soon\n",
      "Scanning category pages: 18/29 - check-in-boarding-122499\n",
      "Scanning category pages: 19/29 - signing-up-for-a-my-cebu-pacific-account-121992\n",
      "Scanning category pages: 20/29 - home\n",
      "Scanning category pages: 21/29 - rebooking-or-canceling-your-flight-120712\n",
      "Scanning category pages: 22/29 - adding-or-linking-a-booking-to-your-account-120798\n",
      "Scanning category pages: 23/29 - updating-your-guest-or-contact-details-122439\n",
      "Scanning category pages: 24/29 - cebu-pacifics-seat-sales-120702\n",
      "Scanning category pages: 25/29 - bangkok-suvarnabhumi-airport-and-travel-guide-186974\n",
      "Scanning category pages: 26/29 - add-ons-services\n",
      "Scanning category pages: 27/29 - brunei-airport-and-travel-guide-190771\n",
      "Scanning category pages: 28/29 - transfer-of-cebgo-dg-and-air-swift-t-220100\n",
      "Scanning category pages: 29/29 - baggage-cargo\n",
      "\n",
      "ğŸ“Š Discovery complete â†’ 238 URLs across 28 categories\n",
      "   [  6]  Add-ons & Services\n",
      "   [ 10]  Adding or Linking a Booking to Your Account\n",
      "   [ 40]  Articles/Flying Out Soon\n",
      "   [  5]  Baggage & Cargo\n",
      "   [  6]  Bali (Denpasar) Airport and Travel Guide\n",
      "   [  6]  Bangkok (Don Mueang) Airport and Travel Guide\n",
      "   [  4]  Bangkok (Suvarnabhumi) Airport and Travel Guide\n",
      "   [ 30]  Booking Flights with Cebu Pacific\n",
      "   [ 16]  Booking a Flight\n",
      "   [  4]  Brunei Airport and Travel Guide\n",
      "   [  6]  CEB Baggage Guidelines and Upgrade Options\n",
      "   [  4]  Can I Bring This in My Checked Baggage?\n",
      "   [  5]  Can I Bring This in My Hand-Carry?\n",
      "   [  8]  Cancelations & Delays\n",
      "   [  3]  Cebu Pacific Domestic Travel Guide\n",
      "   [  1]  Cebu Pacific Enters Damp Lease Agreement with Bulgaria Air\n",
      "   [  1]  Cebu Pacific Launches Riyadh Flights\n",
      "   [  2]  Cebu Pacific's Seat Sales\n",
      "   [ 20]  Check-in & Boarding\n",
      "   [  1]  Guangzhou (Canton) Flights Move to Baiyun Airport Terminal 3\n",
      "   [  9]  Hi, how can we help?\n",
      "   [  2]  How to Contact Cebu Pacific\n",
      "   [  8]  Manage Your Booking\n",
      "   [ 11]  Prepare for Your Flight\n",
      "   [ 17]  Rebooking or Canceling Your Flight\n",
      "   [  6]  Signing up for a MyCebuPacific Account\n",
      "   [  5]  Suspension of Cebu Pacific Flights Between Cebu and San Vicente\n",
      "   [  2]  Updating Your Guest or Contact Details\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 4: URL Discovery â€” Homepage â†’ Category Pages â†’ Article URLs\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "async def discover_article_urls() -> dict:\n",
    "    \"\"\"\n",
    "    Two-depth crawl:\n",
    "      1. Load homepage, collect all internal links\n",
    "      2. Visit each link, collect article URLs + category name\n",
    "    Returns: {category_name: [article_url, ...]}\n",
    "    \"\"\"\n",
    "    domain      = urlparse(BASE_URL).netloc\n",
    "    article_map = defaultdict(set)\n",
    "    visited     = {BASE_URL}\n",
    "\n",
    "    async with async_playwright() as pw:\n",
    "        browser, ctx = await create_stealth_context(pw)\n",
    "        page = await ctx.new_page()\n",
    "        await block_heavy_resources(page)\n",
    "\n",
    "        # â”€â”€ Level 1: Homepage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(f\"ğŸ“¡ Loading homepage...\")\n",
    "        await page.goto(BASE_URL, wait_until=\"load\", timeout=NAV_TIMEOUT)\n",
    "        await asyncio.sleep(random.uniform(1.2, 1.8))\n",
    "\n",
    "        home_html = await page.content()\n",
    "        home_soup = BeautifulSoup(home_html, \"lxml\")\n",
    "\n",
    "        lvl1_links = {\n",
    "            urljoin(BASE_URL, a[\"href\"]).split(\"#\")[0]\n",
    "            for a in home_soup.find_all(\"a\", href=True)\n",
    "            if urlparse(urljoin(BASE_URL, a[\"href\"])).netloc == domain\n",
    "            and urljoin(BASE_URL, a[\"href\"]).split(\"#\")[0] != BASE_URL\n",
    "        }\n",
    "        print(f\"   Found {len(lvl1_links)} links on homepage\")\n",
    "\n",
    "        # â”€â”€ Level 2: Category / Section pages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        for i, link in enumerate(lvl1_links):\n",
    "            print(f\"Scanning category pages: {i+1}/{len(lvl1_links)} - {link.split('/')[-1] or 'home'}\")\n",
    "            if link in visited:\n",
    "                continue\n",
    "            visited.add(link)\n",
    "\n",
    "            try:\n",
    "                await page.goto(link, wait_until=\"load\", timeout=NAV_TIMEOUT)\n",
    "                await asyncio.sleep(random.uniform(MIN_DELAY, MAX_DELAY))\n",
    "                html  = await page.content()\n",
    "                lsoup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "                # Infer category name from page heading\n",
    "                h1 = lsoup.find(\"h1\")\n",
    "                category = (\n",
    "                    h1.get_text(strip=True) if h1\n",
    "                    else urlparse(link).path.strip(\"/\")\n",
    "                                            .replace(\"-\", \" \")\n",
    "                                            .title()\n",
    "                )\n",
    "\n",
    "                # Collect all new internal links on this category page\n",
    "                new_links = {\n",
    "                    urljoin(BASE_URL, a[\"href\"]).split(\"#\")[0]\n",
    "                    for a in lsoup.find_all(\"a\", href=True)\n",
    "                    if urlparse(urljoin(BASE_URL, a[\"href\"])).netloc == domain\n",
    "                    and urljoin(BASE_URL, a[\"href\"]).split(\"#\")[0] not in visited\n",
    "                }\n",
    "                if new_links:\n",
    "                    article_map[category].update(new_links)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Skipped {link}: {str(e)[:60]}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    result = {k: sorted(v) for k, v in article_map.items() if v}\n",
    "    total  = sum(len(v) for v in result.values())\n",
    "    print(f\"\\nğŸ“Š Discovery complete â†’ {total} URLs across {len(result)} categories\")\n",
    "    for cat, urls in sorted(result.items()):\n",
    "        print(f\"   [{len(urls):>3}]  {cat}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "article_url_map = asyncio.run(discover_article_urls())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70ccf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… extract_article() ready\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 5: Article Content Extractor â€” HTML â†’ Clean Structured Text\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def extract_article(html: str) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a rendered article page into clean readable text.\n",
    "    Preserves: headings, paragraphs, bullet lists, numbered lists, tables.\n",
    "    Removes: nav, footer, ads, scripts, forms.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Remove UI noise\n",
    "    for tag in soup.find_all([\"nav\", \"footer\", \"header\", \"aside\",\n",
    "                               \"script\", \"style\", \"noscript\",\n",
    "                               \"form\", \"button\", \"svg\", \"img\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extract title\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = h1.get_text(strip=True) if h1 else (\n",
    "        soup.find(\"title\").get_text(strip=True) if soup.find(\"title\") else \"Untitled\"\n",
    "    )\n",
    "\n",
    "    # Find article body â€” try common content containers in order of preference\n",
    "    body_tag = (\n",
    "        soup.find(\"article\") or\n",
    "        soup.find(attrs={\"class\": re.compile(r\"article|content|body|post\", re.I)}) or\n",
    "        soup.find(\"main\") or\n",
    "        soup.body\n",
    "    )\n",
    "\n",
    "    lines = []\n",
    "    seen  = set()  # Deduplicate repeated navigation text\n",
    "\n",
    "    if body_tag:\n",
    "        for el in body_tag.descendants:\n",
    "            if not hasattr(el, \"name\") or not el.name:\n",
    "                continue\n",
    "            text = el.get_text(strip=True)\n",
    "            if not text or text in seen:\n",
    "                continue\n",
    "\n",
    "            if el.name in (\"h1\", \"h2\", \"h3\", \"h4\"):\n",
    "                seen.add(text)\n",
    "                lines.append(f\"\\n{'#' * int(el.name[1])} {text}\")\n",
    "\n",
    "            elif el.name == \"p\" and len(text) > 15:\n",
    "                seen.add(text)\n",
    "                lines.append(text)\n",
    "\n",
    "            elif el.name == \"li\":\n",
    "                seen.add(text)\n",
    "                bullet = \"- \" if el.find_parent(\"ul\") else f\"{text[:2]}. \".lstrip()\n",
    "                lines.append(f\"{bullet}{text}\")\n",
    "\n",
    "            elif el.name == \"td\" and len(text) > 2:\n",
    "                seen.add(text)\n",
    "                lines.append(f\"| {text}\")\n",
    "\n",
    "    body = re.sub(r\"\\n{3,}\", \"\\n\\n\", \"\\n\".join(lines)).strip()\n",
    "    return {\"title\": title, \"body\": body}\n",
    "\n",
    "\n",
    "print(\"âœ… extract_article() ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5262038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Scraping 238 articles  (concurrency = 4)\n",
      "Scraping: 1/238\n",
      "Scraping: 2/238\n",
      "Scraping: 3/238\n",
      "Scraping: 4/238\n",
      "Scraping: 5/238\n",
      "Scraping: 6/238\n",
      "Scraping: 7/238\n",
      "Scraping: 8/238\n",
      "Scraping: 9/238\n",
      "Scraping: 10/238\n",
      "Scraping: 11/238\n",
      "Scraping: 12/238\n",
      "Scraping: 13/238\n",
      "Scraping: 14/238\n",
      "Scraping: 15/238\n",
      "Scraping: 16/238\n",
      "Scraping: 17/238\n",
      "Scraping: 18/238\n",
      "Scraping: 19/238\n",
      "Scraping: 20/238\n",
      "Scraping: 21/238\n",
      "Scraping: 22/238\n",
      "Scraping: 23/238\n",
      "Scraping: 24/238\n",
      "Scraping: 25/238\n",
      "Scraping: 26/238\n",
      "Scraping: 27/238\n",
      "Scraping: 28/238\n",
      "Scraping: 29/238\n",
      "Scraping: 30/238\n",
      "Scraping: 31/238\n",
      "Scraping: 32/238\n",
      "Scraping: 33/238\n",
      "Scraping: 34/238\n",
      "Scraping: 35/238\n",
      "Scraping: 36/238\n",
      "Scraping: 37/238\n",
      "Scraping: 38/238\n",
      "Scraping: 39/238\n",
      "Scraping: 40/238\n",
      "Scraping: 41/238\n",
      "Scraping: 42/238\n",
      "Scraping: 43/238\n",
      "Scraping: 44/238\n",
      "Scraping: 45/238\n",
      "Scraping: 46/238\n",
      "Scraping: 47/238\n",
      "Scraping: 48/238\n",
      "Scraping: 49/238\n",
      "Scraping: 50/238\n",
      "Scraping: 51/238\n",
      "Scraping: 52/238\n",
      "Scraping: 53/238\n",
      "Scraping: 54/238\n",
      "Scraping: 55/238\n",
      "Scraping: 56/238\n",
      "Scraping: 57/238\n",
      "Scraping: 58/238\n",
      "Scraping: 59/238\n",
      "Scraping: 60/238\n",
      "Scraping: 61/238\n",
      "Scraping: 62/238\n",
      "Scraping: 63/238\n",
      "Scraping: 64/238\n",
      "Scraping: 65/238\n",
      "Scraping: 66/238\n",
      "Scraping: 67/238\n",
      "Scraping: 68/238\n",
      "Scraping: 69/238\n",
      "Scraping: 70/238\n",
      "Scraping: 71/238\n",
      "Scraping: 72/238\n",
      "Scraping: 73/238\n",
      "Scraping: 74/238\n",
      "Scraping: 75/238\n",
      "Scraping: 76/238\n",
      "Scraping: 77/238\n",
      "Scraping: 78/238\n",
      "Scraping: 79/238\n",
      "Scraping: 80/238\n",
      "Scraping: 81/238\n",
      "Scraping: 82/238\n",
      "Scraping: 83/238\n",
      "Scraping: 84/238\n",
      "Scraping: 85/238\n",
      "Scraping: 86/238\n",
      "Scraping: 87/238\n",
      "Scraping: 88/238\n",
      "Scraping: 89/238\n",
      "Scraping: 90/238\n",
      "Scraping: 91/238\n",
      "Scraping: 92/238\n",
      "Scraping: 93/238\n",
      "Scraping: 94/238\n",
      "Scraping: 95/238\n",
      "Scraping: 96/238\n",
      "Scraping: 97/238\n",
      "Scraping: 98/238\n",
      "Scraping: 99/238\n",
      "Scraping: 100/238\n",
      "Scraping: 101/238\n",
      "Scraping: 102/238\n",
      "Scraping: 103/238\n",
      "Scraping: 104/238\n",
      "Scraping: 105/238\n",
      "Scraping: 106/238\n",
      "Scraping: 107/238\n",
      "Scraping: 108/238\n",
      "Scraping: 109/238\n",
      "Scraping: 110/238\n",
      "Scraping: 111/238\n",
      "Scraping: 112/238\n",
      "Scraping: 113/238\n",
      "Scraping: 114/238\n",
      "Scraping: 115/238\n",
      "Scraping: 116/238\n",
      "Scraping: 117/238\n",
      "Scraping: 118/238\n",
      "Scraping: 119/238\n",
      "Scraping: 120/238\n",
      "Scraping: 121/238\n",
      "Scraping: 122/238\n",
      "Scraping: 123/238\n",
      "Scraping: 124/238\n",
      "Scraping: 125/238\n",
      "Scraping: 126/238\n",
      "Scraping: 127/238\n",
      "Scraping: 128/238\n",
      "Scraping: 129/238\n",
      "Scraping: 130/238\n",
      "Scraping: 131/238\n",
      "Scraping: 132/238\n",
      "Scraping: 133/238\n",
      "Scraping: 134/238\n",
      "Scraping: 135/238\n",
      "Scraping: 136/238\n",
      "Scraping: 137/238\n",
      "Scraping: 138/238\n",
      "Scraping: 139/238\n",
      "Scraping: 140/238\n",
      "Scraping: 141/238\n",
      "Scraping: 142/238\n",
      "Scraping: 143/238\n",
      "Scraping: 144/238\n",
      "Scraping: 145/238\n",
      "Scraping: 146/238\n",
      "Scraping: 147/238\n",
      "Scraping: 148/238\n",
      "Scraping: 149/238\n",
      "Scraping: 150/238\n",
      "Scraping: 151/238\n",
      "Scraping: 152/238\n",
      "Scraping: 153/238\n",
      "Scraping: 154/238\n",
      "Scraping: 155/238\n",
      "Scraping: 156/238\n",
      "Scraping: 157/238\n",
      "Scraping: 158/238\n",
      "Scraping: 159/238\n",
      "Scraping: 160/238\n",
      "Scraping: 161/238\n",
      "Scraping: 162/238\n",
      "Scraping: 163/238\n",
      "Scraping: 164/238\n",
      "Scraping: 165/238\n",
      "Scraping: 166/238\n",
      "Scraping: 167/238\n",
      "Scraping: 168/238\n",
      "Scraping: 169/238\n",
      "Scraping: 170/238\n",
      "Scraping: 171/238\n",
      "Scraping: 172/238\n",
      "Scraping: 173/238\n",
      "Scraping: 174/238\n",
      "Scraping: 175/238\n",
      "Scraping: 176/238\n",
      "Scraping: 177/238\n",
      "Scraping: 178/238\n",
      "Scraping: 179/238\n",
      "Scraping: 180/238\n",
      "Scraping: 181/238\n",
      "Scraping: 182/238\n",
      "Scraping: 183/238\n",
      "Scraping: 184/238\n",
      "Scraping: 185/238\n",
      "Scraping: 186/238\n",
      "Scraping: 187/238\n",
      "Scraping: 188/238\n",
      "Scraping: 189/238\n",
      "Scraping: 190/238\n",
      "Scraping: 191/238\n",
      "Scraping: 192/238\n",
      "Scraping: 193/238\n",
      "Scraping: 194/238\n",
      "Scraping: 195/238\n",
      "Scraping: 196/238\n",
      "Scraping: 197/238\n",
      "Scraping: 198/238\n",
      "Scraping: 199/238\n",
      "Scraping: 200/238\n",
      "Scraping: 201/238\n",
      "Scraping: 202/238\n",
      "Scraping: 203/238\n",
      "Scraping: 204/238\n",
      "Scraping: 205/238\n",
      "Scraping: 206/238\n",
      "Scraping: 207/238\n",
      "Scraping: 208/238\n",
      "Scraping: 209/238\n",
      "Scraping: 210/238\n",
      "Scraping: 211/238\n",
      "Scraping: 212/238\n",
      "Scraping: 213/238\n",
      "Scraping: 214/238\n",
      "Scraping: 215/238\n",
      "Scraping: 216/238\n",
      "Scraping: 217/238\n",
      "Scraping: 218/238\n",
      "Scraping: 219/238\n",
      "Scraping: 220/238\n",
      "Scraping: 221/238\n",
      "Scraping: 222/238\n",
      "Scraping: 223/238\n",
      "Scraping: 224/238\n",
      "Scraping: 225/238\n",
      "Scraping: 226/238\n",
      "Scraping: 227/238\n",
      "Scraping: 228/238\n",
      "Scraping: 229/238\n",
      "Scraping: 230/238\n",
      "Scraping: 231/238\n",
      "Scraping: 232/238\n",
      "Scraping: 233/238\n",
      "Scraping: 234/238\n",
      "Scraping: 235/238\n",
      "Scraping: 236/238\n",
      "Scraping: 237/238\n",
      "Scraping: 238/238\n",
      "\n",
      "âœ… 199/238 articles scraped successfully\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 6: Async Batch Scraper â€” Concurrent Article Fetching\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "async def scrape_one(ctx: BrowserContext, url: str,\n",
    "                     category: str, sem: asyncio.Semaphore) -> dict | None:\n",
    "    \"\"\"Scrape a single article â€” one tab per article, semaphore-limited.\"\"\"\n",
    "    async with sem:\n",
    "        page = await ctx.new_page()\n",
    "        await block_heavy_resources(page)\n",
    "        try:\n",
    "            await page.goto(url, wait_until=\"load\", timeout=NAV_TIMEOUT)\n",
    "            await asyncio.sleep(random.uniform(MIN_DELAY, MAX_DELAY))\n",
    "            content = extract_article(await page.content())\n",
    "            if not content[\"body\"]:\n",
    "                return None\n",
    "            return {\n",
    "                \"url\":      url,\n",
    "                \"category\": category,\n",
    "                \"title\":    content[\"title\"],\n",
    "                \"body\":     content[\"body\"],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {url}: {str(e)[:70]}\")\n",
    "            return None\n",
    "        finally:\n",
    "            await page.close()\n",
    "\n",
    "\n",
    "async def scrape_all(url_map: dict) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run all article scrapes concurrently under a shared semaphore.\n",
    "    Uses a single browser + context for connection reuse (fast + stealthy).\n",
    "    \"\"\"\n",
    "    flat = [(cat, url) for cat, urls in url_map.items() for url in urls]\n",
    "    print(f\"ğŸš€ Scraping {len(flat)} articles  (concurrency = {CONCURRENCY})\")\n",
    "\n",
    "    results = []\n",
    "    sem = asyncio.Semaphore(CONCURRENCY)\n",
    "\n",
    "    async with async_playwright() as pw:\n",
    "        browser, ctx = await create_stealth_context(pw)\n",
    "        coros = [scrape_one(ctx, url, cat, sem) for cat, url in flat]\n",
    "\n",
    "        for i, fut in enumerate(asyncio.as_completed(coros)):\n",
    "            print(f\"Scraping: {i+1}/{len(coros)}\")\n",
    "            res = await fut\n",
    "            if res:\n",
    "                results.append(res)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"\\nâœ… {len(results)}/{len(flat)} articles scraped successfully\")\n",
    "    return results\n",
    "\n",
    "\n",
    "scraped_articles = asyncio.run(scrape_all(article_url_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe8fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved â†’ /Users/twotabs/Documents/GitHub/TwoTabs_James_Dev/DsPy-Learning-Repo/Cebu_Pacific_Ticket_Agent_V2/cebu_pacific_helpcenter.txt\n",
      "   Size      : 820.3 KB\n",
      "   Articles  : 199\n",
      "   Categories: 23\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 7: Save All Content â†’ Single Master TXT File\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def save_master_txt(articles: list[dict], path: str = OUTPUT_FILE) -> Path:\n",
    "    \"\"\"\n",
    "    Write all scraped articles to one structured txt file.\n",
    "    Format is designed so you can later split it into JSONL train/val sets.\n",
    "\n",
    "    Structure per article:\n",
    "        [INDEX] TITLE\n",
    "        URL      : ...\n",
    "        CATEGORY : ...\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        [body text]\n",
    "        â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\"\n",
    "    out = Path(path)\n",
    "\n",
    "    header = (\n",
    "        \"=\" * 80 + \"\\n\"\n",
    "        f\"SOURCE   : {BASE_URL}\\n\"\n",
    "        f\"SCRAPED  : {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\"\n",
    "        f\"ARTICLES : {len(articles)}\\n\"\n",
    "        \"=\" * 80 + \"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    by_cat = defaultdict(list)\n",
    "    for a in articles:\n",
    "        by_cat[a[\"category\"]].append(a)\n",
    "\n",
    "    blocks = [header]\n",
    "    idx    = 0\n",
    "\n",
    "    for category in sorted(by_cat):\n",
    "        blocks.append(f\"\\n{'â–ˆ' * 80}\\nâ–Œ CATEGORY: {category.upper()}\\n{'â–ˆ' * 80}\\n\")\n",
    "        for art in by_cat[category]:\n",
    "            idx += 1\n",
    "            blocks.append(\n",
    "                f\"\\n[{idx:03d}] {art['title']}\\n\"\n",
    "                f\"URL      : {art['url']}\\n\"\n",
    "                f\"CATEGORY : {art['category']}\\n\"\n",
    "                + \"-\" * 60 + \"\\n\"\n",
    "                + art[\"body\"] + \"\\n\"\n",
    "                + \"=\" * 80 + \"\\n\"\n",
    "            )\n",
    "\n",
    "    out.write_text(\"\".join(blocks), encoding=\"utf-8\")\n",
    "    kb = out.stat().st_size / 1024\n",
    "    print(f\"âœ… Saved â†’ {out.resolve()}\")\n",
    "    print(f\"   Size      : {kb:.1f} KB\")\n",
    "    print(f\"   Articles  : {len(articles)}\")\n",
    "    print(f\"   Categories: {len(by_cat)}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "output_path = save_master_txt(scraped_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d4a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“‹ SCRAPE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Category                                   Articles\n",
      "----------------------------------------------------\n",
      "  Articles/Flying Out Soon                       39\n",
      "  Booking Flights with Cebu Pacific              28\n",
      "  Check-in & Boarding                            17\n",
      "  Rebooking or Canceling Your Flight             16\n",
      "  Booking a Flight                               15\n",
      "  Prepare for Your Flight                        10\n",
      "  Adding or Linking a Booking to Your Account        9\n",
      "  Cancelations & Delays                           7\n",
      "  Manage Your Booking                             7\n",
      "  Hi, how can we help?                            7\n",
      "  Add-ons & Services                              6\n",
      "  Baggage & Cargo                                 5\n",
      "  CEB Baggage Guidelines and Upgrade Options        4\n",
      "  Suspension of Cebu Pacific Flights Between Cebu and San Vicente        4\n",
      "  Bangkok (Don Mueang) Airport and Travel Guide        4\n",
      "  Signing up for a MyCebuPacific Account          4\n",
      "  Bali (Denpasar) Airport and Travel Guide        4\n",
      "  Bangkok (Suvarnabhumi) Airport and Travel Guide        3\n",
      "  Can I Bring This in My Hand-Carry?              3\n",
      "  Brunei Airport and Travel Guide                 3\n",
      "  Can I Bring This in My Checked Baggage?         2\n",
      "  Cebu Pacific Domestic Travel Guide              1\n",
      "  Updating Your Guest or Contact Details          1\n",
      "\n",
      "ğŸ“Š Content Stats:\n",
      "   Total articles    : 199\n",
      "   Avg body length   : 3,777 chars\n",
      "   Min body length   : 190 chars\n",
      "   Max body length   : 22,497 chars\n",
      "\n",
      "ğŸ“ Sample Article:\n",
      "   Title    : Payment and Receipt Concerns\n",
      "   Category : Booking Flights with Cebu Pacific\n",
      "   URL      : https://help.cebupacificair.com/article/122336\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "# Payment and Receipt Concerns\n",
      "Learn more about the Itinerary Receipt, Invoice and your options when paying for flights and add-ons.\n",
      "\n",
      "## Overview\n",
      "\n",
      "### Description\n",
      "\n",
      "## ğŸ’µ Accepted Forms of Payment\n",
      "You can pay for your flight or add-ons with the following options:\n",
      "\n",
      "## âœˆï¸ Go Rewards\n",
      "You can redeem or use your Go Rewards points in the Cebu Pacific app or website.\n",
      "- Learn more here:How to Earn and Redeem Go Rewards Points\n",
      "\n",
      "## ğŸª™ Travel Fund\n",
      "The Travel Fund is a virtual wallet and can be used to pay for...\n",
      "\n",
      "ğŸ¯ Master file ready: cebu_pacific_helpcenter.txt\n",
      "   Next step: Use this file to build accurate JSONL train/val sets!\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 8: Verification â€” Stats + Sample Preview\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ SCRAPE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cat_counts   = Counter(a[\"category\"] for a in scraped_articles)\n",
    "body_lengths = [len(a[\"body\"]) for a in scraped_articles]\n",
    "\n",
    "print(f\"\\n{'Category':<42} {'Articles':>8}\")\n",
    "print(\"-\" * 52)\n",
    "for cat, n in sorted(cat_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cat:<40} {n:>8}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Content Stats:\")\n",
    "print(f\"   Total articles    : {len(scraped_articles)}\")\n",
    "print(f\"   Avg body length   : {sum(body_lengths) // len(body_lengths):,} chars\")\n",
    "print(f\"   Min body length   : {min(body_lengths):,} chars\")\n",
    "print(f\"   Max body length   : {max(body_lengths):,} chars\")\n",
    "\n",
    "# Sample preview\n",
    "sample = scraped_articles[0]\n",
    "print(f\"\\nğŸ“ Sample Article:\")\n",
    "print(f\"   Title    : {sample['title']}\")\n",
    "print(f\"   Category : {sample['category']}\")\n",
    "print(f\"   URL      : {sample['url']}\")\n",
    "print(f\"\\n{'â”€'*60}\")\n",
    "print(sample[\"body\"][:500] + \"...\")\n",
    "print(f\"\\nğŸ¯ Master file ready: {OUTPUT_FILE}\")\n",
    "print(\"   Next step: Use this file to build accurate JSONL train/val sets!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
