{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433d0753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… .env loaded from /Users/twotabs/Documents/GitHub/TwoTabs_James_Dev/.env\n",
      "âœ… Config ready\n",
      "   API Key  : gsk_wtFg...yQtI\n",
      "   Model    : llama-3.1-8b-instant\n",
      "   Examples : 5 per article  (was 2)\n",
      "   Est. raw : ~880 examples  (was ~352)\n",
      "   Split    : 80% train / 19% val\n",
      "   Max body : 2000 chars/article  (was 1200)\n",
      "   Opt dir  : /Users/twotabs/Documents/GitHub/TwoTabs_James_Dev/DsPy-Learning-Repo/Customer_Ticket_Support_Agent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 1: Imports & Configuration\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import os, re, json, asyncio, random, shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from groq import AsyncGroq\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load .env from workspace root (2 levels up from this notebook)\n",
    "_env_path = Path(\"../../.env\")\n",
    "if _env_path.exists():\n",
    "    load_dotenv(_env_path)\n",
    "    print(f\"âœ… .env loaded from {_env_path.resolve()}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  .env not found at {_env_path.resolve()}, falling back to environment variables\")\n",
    "\n",
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GROQ_API_KEY    = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "HELPCENTER_FILE = \"cebu_pacific_helpcenter.txt\"\n",
    "TRAINSET_OUT    = \"cebu_pacific_trainset_v2.jsonl\"\n",
    "VALSET_OUT      = \"cebu_pacific_valset_v2.jsonl\"\n",
    "\n",
    "# Output dir for the optimization pipeline (copies will be placed here)\n",
    "OPT_DIR = Path(\"../Customer_Ticket_Support_Agent\")\n",
    "\n",
    "# âš¡ Token-optimized: use fast 8B model (â‰ˆ15Ã— cheaper than 70B)\n",
    "GENERATION_MODEL    = \"llama-3.1-8b-instant\"\n",
    "\n",
    "# ğŸ“ˆ Enrichment settings â€” 5 diverse personas per article Ã— 176 articles = 880 raw\n",
    "QUERIES_PER_ARTICLE = 5        # 5 distinct persona types per article (was 2)\n",
    "CONCURRENCY         = 3        # Conservative â€” stays within Groq rate limits\n",
    "TRAIN_RATIO         = 0.80\n",
    "MAX_BODY_CHARS      = 2000     # More article context â†’ better-grounded resolutions (was 1200)\n",
    "\n",
    "assert GROQ_API_KEY, \"âŒ GROQ_API_KEY not found â€” check your .env file\"\n",
    "\n",
    "est_raw = 176 * QUERIES_PER_ARTICLE\n",
    "print(\"âœ… Config ready\")\n",
    "print(f\"   API Key  : {GROQ_API_KEY[:8]}...{GROQ_API_KEY[-4:]}\")\n",
    "print(f\"   Model    : {GENERATION_MODEL}\")\n",
    "print(f\"   Examples : {QUERIES_PER_ARTICLE} per article  (was 2)\")\n",
    "print(f\"   Est. raw : ~{est_raw} examples  (was ~352)\")\n",
    "print(f\"   Split    : {int(TRAIN_RATIO*100)}% train / {int((1-TRAIN_RATIO)*100)}% val\")\n",
    "print(f\"   Max body : {MAX_BODY_CHARS} chars/article  (was 1200)\")\n",
    "print(f\"   Opt dir  : {OPT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8899de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parsed 176 articles from cebu_pacific_helpcenter.txt\n",
      "\n",
      "Category                                 Articles\n",
      "--------------------------------------------------\n",
      "  Articles/Flying Out Soon                     38\n",
      "  Booking Flights with Cebu Pacific            27\n",
      "  Check-in & Boarding                          16\n",
      "  Rebooking or Canceling Your Flight           15\n",
      "  Booking a Flight                             14\n",
      "  Prepare for Your Flight                       9\n",
      "  Adding or Linking a Booking to Your Account        8\n",
      "  Cancelations & Delays                         6\n",
      "  Hi, how can we help?                          6\n",
      "  Manage Your Booking                           6\n",
      "  Add-ons & Services                            5\n",
      "  Baggage & Cargo                               4\n",
      "  Bali (Denpasar) Airport and Travel Guide        3\n",
      "  Bangkok (Don Mueang) Airport and Travel Guide        3\n",
      "  CEB Baggage Guidelines and Upgrade Options        3\n",
      "  Signing up for a MyCebuPacific Account        3\n",
      "  Suspension of Cebu Pacific Flights Between Cebu and San Vicente        3\n",
      "  Bangkok (Suvarnabhumi) Airport and Travel Guide        2\n",
      "  Brunei Airport and Travel Guide               2\n",
      "  Can I Bring This in My Hand-Carry?            2\n",
      "  Can I Bring This in My Checked Baggage?        1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 3: Parse helpcenter.txt â†’ List of Article Dicts\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def parse_helpcenter(filepath: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse the master TXT file into individual article dicts.\n",
    "    Handles the [001] TITLE / URL / CATEGORY / body structure.\n",
    "    \"\"\"\n",
    "    text    = Path(filepath).read_text(encoding=\"utf-8\")\n",
    "    chunks  = re.split(r\"={60,}\", text)   # Split by â•â• separator\n",
    "    articles = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk or not re.match(r\"\\[\\d+\\]\", chunk):\n",
    "            continue\n",
    "\n",
    "        lines = chunk.split(\"\\n\")\n",
    "\n",
    "        # Header line: [001] Article Title\n",
    "        header_match = re.match(r\"\\[(\\d+)\\]\\s+(.+)\", lines[0].strip())\n",
    "        if not header_match:\n",
    "            continue\n",
    "        idx, title = header_match.groups()\n",
    "\n",
    "        url = category = \"\"\n",
    "        body_start_idx = 0\n",
    "\n",
    "        for i, line in enumerate(lines[1:], 1):\n",
    "            if line.startswith(\"URL\"):\n",
    "                url = re.split(r\":\\s+\", line, maxsplit=1)[-1].strip()\n",
    "            elif line.startswith(\"CATEGORY\"):\n",
    "                category = re.split(r\":\\s+\", line, maxsplit=1)[-1].strip()\n",
    "            elif re.match(r\"^-{20,}$\", line.strip()):\n",
    "                body_start_idx = i + 1\n",
    "                break\n",
    "\n",
    "        body = \"\\n\".join(lines[body_start_idx:]).strip()\n",
    "\n",
    "        # Clean trailing noise (related articles section, feedback widget)\n",
    "        body = re.sub(r\"\\n#{1,3}\\s*Related articles.*$\", \"\", body, flags=re.DOTALL)\n",
    "        body = re.sub(r\"Did you find this article helpful\\?.*$\", \"\", body, flags=re.DOTALL)\n",
    "        body = re.sub(r\"\\n{3,}\", \"\\n\\n\", body).strip()\n",
    "\n",
    "        if len(body) < 50:    # Skip empty/stub articles\n",
    "            continue\n",
    "\n",
    "        articles.append({\n",
    "            \"idx\":      int(idx),\n",
    "            \"title\":    title.strip(),\n",
    "            \"url\":      url,\n",
    "            \"category\": category,\n",
    "            \"body\":     body,\n",
    "        })\n",
    "\n",
    "    print(f\"âœ… Parsed {len(articles)} articles from {filepath}\")\n",
    "\n",
    "    by_cat = defaultdict(int)\n",
    "    for a in articles:\n",
    "        by_cat[a[\"category\"]] += 1\n",
    "    print(f\"\\n{'Category':<40} {'Articles':>8}\")\n",
    "    print(\"-\" * 50)\n",
    "    for cat, n in sorted(by_cat.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {cat:<38} {n:>8}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "articles = parse_helpcenter(HELPCENTER_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5962516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enriched prompt + generator ready\n",
      "   Model       : llama-3.1-8b-instant\n",
      "   Max tokens  : 2800 per call  (was 1400)\n",
      "   Max body    : 2000 chars  (was 1200)\n",
      "   Personas    : 5 distinct styles per article\n",
      "   Temperature : 0.85  (higher for style variety)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 4: Prompt Template & Async Generator  (diverse-persona, enriched)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a dataset engineer building a Cebu Pacific support AI training set. \"\n",
    "    \"Output valid JSON only â€” no markdown fences, no commentary.\"\n",
    ")\n",
    "\n",
    "# Five distinct passenger personas to maximise query diversity\n",
    "PERSONAS = \"\"\"PERSONA GUIDE â€” use each persona exactly once across your {n} examples:\n",
    "1. FRUSTRATED/URGENT  â€” ALL CAPS, multiple !!!, genuine panic (e.g. \"flight is TOMORROW help!!!\")\n",
    "2. CONFUSED NEWCOMER  â€” first-time flyer, uncertain wording, asks very basic questions\n",
    "3. TAGALOG-ENGLISH    â€” natural code-switch (e.g. \"pwede ba mag-rebook\", \"magkano ang bayad\")\n",
    "4. POLITE FORMAL      â€” complete sentences, proper grammar, professional tone\n",
    "5. POLICY CHALLENGE   â€” questions the fairness of a fee/rule, mild escalation language\"\"\"\n",
    "\n",
    "\n",
    "def build_prompt(article: dict, n: int) -> str:\n",
    "    body = article[\"body\"][:MAX_BODY_CHARS]\n",
    "    personas = PERSONAS.format(n=n)\n",
    "    return f\"\"\"Generate {n} diverse customer support training examples from this Cebu Pacific help article.\n",
    "\n",
    "CATEGORY: {article['category']}\n",
    "TITLE: {article['title']}\n",
    "CONTENT:\n",
    "{body}\n",
    "\n",
    "{personas}\n",
    "\n",
    "Field rules:\n",
    "- customer_query   : 50-200 chars. Must sound like a real passenger message for that persona.\n",
    "                     Ask about something SPECIFIC in the article (not just restate the title).\n",
    "- issue_category   : snake_case slug describing the problem (e.g. baggage_allowance_inquiry,\n",
    "                     online_checkin_failure, refund_request, rebooking_fee_dispute)\n",
    "- resolution       : 400-900 chars. Structured with bullet points or numbered steps.\n",
    "                     Ground EVERY policy claim in the article content above.\n",
    "                     Mention specific fees, timelines, or contact channels where present.\n",
    "                     Close with a next-step or follow-up question for the passenger.\n",
    "- resolution_time_minutes : integer 3-12\n",
    "\n",
    "Return a JSON array of exactly {n} objects. Nothing else.\n",
    "[{{\"customer_query\":\"...\",\"issue_category\":\"...\",\"resolution\":\"...\",\"resolution_time_minutes\":5}}]\"\"\"\n",
    "\n",
    "\n",
    "async def generate_for_article(\n",
    "    client: AsyncGroq,\n",
    "    article: dict,\n",
    "    sem: asyncio.Semaphore,\n",
    "    n: int = 5,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Generate n persona-diverse examples for one article with retry + backoff.\"\"\"\n",
    "    async with sem:\n",
    "        for attempt in range(4):\n",
    "            try:\n",
    "                resp = await client.chat.completions.create(\n",
    "                    model=GENERATION_MODEL,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\",   \"content\": build_prompt(article, n)},\n",
    "                    ],\n",
    "                    temperature=0.85,   # Higher temp â†’ more varied phrasing across personas\n",
    "                    max_tokens=2800,    # 5 examples Ã— ~550 chars resolution = ~2750 needed\n",
    "                )\n",
    "                raw = resp.choices[0].message.content.strip()\n",
    "\n",
    "                # Strip accidental markdown fences\n",
    "                raw = re.sub(r\"^```(?:json)?\\s*\", \"\", raw)\n",
    "                raw = re.sub(r\"\\s*```$\", \"\", raw)\n",
    "\n",
    "                match = re.search(r\"\\[.*\\]\", raw, re.DOTALL)\n",
    "                if not match:\n",
    "                    raise ValueError(\"No JSON array found in response\")\n",
    "\n",
    "                examples = json.loads(match.group())\n",
    "\n",
    "                for ex in examples:\n",
    "                    ex[\"source_article\"] = article[\"title\"]\n",
    "                    ex[\"source_url\"]     = article[\"url\"]\n",
    "                    ex[\"category\"]       = article[\"category\"]\n",
    "\n",
    "                return examples\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == 3:\n",
    "                    print(f\"   âŒ Skipped [{article['idx']:03d}] \"\n",
    "                          f\"{article['title'][:45]}: {str(e)[:60]}\")\n",
    "                wait = 2 ** attempt + random.uniform(0, 1.5)\n",
    "                await asyncio.sleep(wait)\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"âœ… Enriched prompt + generator ready\")\n",
    "print(f\"   Model       : {GENERATION_MODEL}\")\n",
    "print(f\"   Max tokens  : 2800 per call  (was 1400)\")\n",
    "print(f\"   Max body    : {MAX_BODY_CHARS} chars  (was 1200)\")\n",
    "print(f\"   Personas    : 5 distinct styles per article\")\n",
    "print(f\"   Temperature : 0.85  (higher for style variety)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bece597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Generating ~880 examples from 176 articles\n",
      "   Model: llama-3.1-8b-instant  |  Concurrency: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e406f313dd4f8cbee565147480f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Articles processed:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Skipped [008] Signing up for a MyCebuPacific Account: Invalid control character at: line 5 column 510 (char 684)\n",
      "   âŒ Skipped [033] AirSWIFT Airlines Migrates to Cebu Pacific Sy: Expecting ',' delimiter: line 23 column 142 (char 2495)\n",
      "\n",
      "âœ… Raw generation complete: 873 examples\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 5: Run Async Batch Generation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "async def generate_full_dataset(articles: list[dict]) -> list[dict]:\n",
    "    client     = AsyncGroq(api_key=GROQ_API_KEY)\n",
    "    sem        = asyncio.Semaphore(CONCURRENCY)\n",
    "    all_results = []\n",
    "\n",
    "    estimated  = len(articles) * QUERIES_PER_ARTICLE\n",
    "    print(f\"ğŸš€ Generating ~{estimated} examples from {len(articles)} articles\")\n",
    "    print(f\"   Model: {GENERATION_MODEL}  |  Concurrency: {CONCURRENCY}\\n\")\n",
    "\n",
    "    coros = [\n",
    "        generate_for_article(client, art, sem, QUERIES_PER_ARTICLE)\n",
    "        for art in articles\n",
    "    ]\n",
    "\n",
    "    for fut in tqdm(\n",
    "        asyncio.as_completed(coros),\n",
    "        total=len(coros),\n",
    "        desc=\"Articles processed\"\n",
    "    ):\n",
    "        results = await fut\n",
    "        all_results.extend(results)\n",
    "\n",
    "    print(f\"\\nâœ… Raw generation complete: {len(all_results)} examples\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "raw_examples = asyncio.run(generate_full_dataset(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a852a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Quality filter: 873 â†’ 783 examples kept\n",
      "   Removed reasons:\n",
      "   - resolution_too_short     : 44\n",
      "   - query_length             : 27\n",
      "   - duplicate                : 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 6: Quality Filter + Deduplication  (thresholds aligned to enriched prompt)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def quality_filter(examples: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Keep examples that:\n",
    "    - Have all required fields\n",
    "    - Query is 40â€“250 chars (covers all 5 persona styles including long Tagalog-English)\n",
    "    - Resolution is â‰¥300 chars (aligned with 400-900 char prompt target)\n",
    "    - issue_category is a plausible snake_case slug (not blank, not a full sentence)\n",
    "    - No near-duplicate queries (first 50 chars unique)\n",
    "    \"\"\"\n",
    "    required = {\"customer_query\", \"issue_category\", \"resolution\", \"resolution_time_minutes\"}\n",
    "    seen_queries = set()\n",
    "    passed = []\n",
    "    reasons = defaultdict(int)\n",
    "\n",
    "    def is_valid_slug(s: str) -> bool:\n",
    "        # Must look like snake_case, not a sentence\n",
    "        return bool(re.match(r\"^[a-z][a-z0-9_]{2,49}$\", s))\n",
    "\n",
    "    for ex in examples:\n",
    "        if not required.issubset(ex.keys()):\n",
    "            reasons[\"missing_fields\"] += 1\n",
    "            continue\n",
    "\n",
    "        q = str(ex[\"customer_query\"]).strip()\n",
    "        r = str(ex[\"resolution\"]).strip()\n",
    "        cat = str(ex[\"issue_category\"]).strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "        # Length checks\n",
    "        if not (40 <= len(q) <= 250):\n",
    "            reasons[\"query_length\"] += 1\n",
    "            continue\n",
    "        if len(r) < 300:\n",
    "            reasons[\"resolution_too_short\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Category slug sanity\n",
    "        if not is_valid_slug(cat):\n",
    "            reasons[\"bad_issue_category\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Dedup on first 50 chars of query (wider window catches more near-dups)\n",
    "        key = q[:50].lower()\n",
    "        if key in seen_queries:\n",
    "            reasons[\"duplicate\"] += 1\n",
    "            continue\n",
    "        seen_queries.add(key)\n",
    "\n",
    "        passed.append({\n",
    "            \"customer_query\":          q,\n",
    "            \"issue_category\":          cat,\n",
    "            \"resolution\":              r,\n",
    "            \"resolution_time_minutes\": int(ex.get(\"resolution_time_minutes\", 5)),\n",
    "            \"source_article\":          ex.get(\"source_article\", \"\"),\n",
    "            \"source_url\":              ex.get(\"source_url\", \"\"),\n",
    "            \"category\":                ex.get(\"category\", \"\"),\n",
    "        })\n",
    "\n",
    "    print(f\"âœ… Quality filter: {len(examples)} â†’ {len(passed)} examples kept\")\n",
    "    if reasons:\n",
    "        print(f\"   Removed reasons:\")\n",
    "        for reason, count in sorted(reasons.items(), key=lambda x: -x[1]):\n",
    "            print(f\"   - {reason:<25}: {count}\")\n",
    "\n",
    "    return passed\n",
    "\n",
    "\n",
    "filtered_examples = quality_filter(raw_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff125a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“‹ Copied â†’ ../Customer_Ticket_Support_Agent/cebu_pacific_trainset.jsonl\n",
      "   ğŸ“‹ Copied â†’ ../Customer_Ticket_Support_Agent/cebu_pacific_valset.jsonl\n",
      "\n",
      "âœ… Dataset saved!\n",
      "   Train  :  626 examples â†’ cebu_pacific_trainset_v2.jsonl\n",
      "   Val    :  157 examples â†’ cebu_pacific_valset_v2.jsonl\n",
      "   Total  :  783 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 7: Shuffle â†’ Train/Val Split â†’ Save as JSONL + Copy to Opt Pipeline\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def save_split(examples: list[dict], train_path: str, val_path: str) -> tuple:\n",
    "    random.seed(42)        # Reproducible shuffle\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    split_idx = int(len(examples) * TRAIN_RATIO)\n",
    "    train_set = examples[:split_idx]\n",
    "    val_set   = examples[split_idx:]\n",
    "\n",
    "    def add_ids(dataset, prefix):\n",
    "        for i, ex in enumerate(dataset, 1):\n",
    "            ex[\"ticket_id\"] = f\"CP-2026-{prefix}-{i:04d}\"\n",
    "        return dataset\n",
    "\n",
    "    train_set = add_ids(train_set, \"TR\")\n",
    "    val_set   = add_ids(val_set,   \"VL\")\n",
    "\n",
    "    def write_jsonl(path, data):\n",
    "        Path(path).write_text(\n",
    "            \"\\n\".join(json.dumps(ex, ensure_ascii=False) for ex in data),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    # â”€â”€ Save v2 files in this directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    write_jsonl(train_path, train_set)\n",
    "    write_jsonl(val_path,   val_set)\n",
    "\n",
    "    # â”€â”€ Copy to Customer_Ticket_Support_Agent so optimizer picks them up â”€â”€â”€\n",
    "    if OPT_DIR.exists():\n",
    "        opt_train = OPT_DIR / \"cebu_pacific_trainset.jsonl\"\n",
    "        opt_val   = OPT_DIR / \"cebu_pacific_valset.jsonl\"\n",
    "        shutil.copy(train_path, opt_train)\n",
    "        shutil.copy(val_path,   opt_val)\n",
    "        print(f\"   ğŸ“‹ Copied â†’ {opt_train}\")\n",
    "        print(f\"   ğŸ“‹ Copied â†’ {opt_val}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  OPT_DIR not found ({OPT_DIR}), skipping copy\")\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "trainset_v2, valset_v2 = save_split(filtered_examples, TRAINSET_OUT, VALSET_OUT)\n",
    "\n",
    "print(f\"\\nâœ… Dataset saved!\")\n",
    "print(f\"   Train  : {len(trainset_v2):>4} examples â†’ {TRAINSET_OUT}\")\n",
    "print(f\"   Val    : {len(valset_v2):>4} examples â†’ {VALSET_OUT}\")\n",
    "print(f\"   Total  : {len(trainset_v2) + len(valset_v2):>4} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab1a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“‹ ENRICHED DATASET v2 SUMMARY\n",
      "======================================================================\n",
      "\n",
      "   Total examples : 783\n",
      "   Train          : 626\n",
      "   Val            : 157\n",
      "\n",
      "Issue Category                            Count      %\n",
      "-------------------------------------------------------\n",
      "  rebooking_fee_dispute                     168  21.5%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  baggage_allowance_inquiry                  64   8.2%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  online_checkin_failure                     35   4.5%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  refund_request                             34   4.3%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  rebooking_fee_inquiry                      25   3.2%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  visa_requirement_inquiry                    6   0.8%  â–ˆ\n",
      "  flight_status_inquiry                       5   0.6%  â–ˆ\n",
      "  baggage_fee_dispute                         4   0.5%  â–ˆ\n",
      "  rebooking_request                           4   0.5%  â–ˆ\n",
      "  online_checkin_fee_inquiry                  4   0.5%  â–ˆ\n",
      "  policy_challenge                            4   0.5%  â–ˆ\n",
      "  flight_change_request                       4   0.5%  â–ˆ\n",
      "  missing_itinerary_receipt                   3   0.4%  \n",
      "  lost_airport_item                           3   0.4%  \n",
      "  rebooking_fee                               3   0.4%  \n",
      "  rebooking_destination_change                3   0.4%  \n",
      "  airport_facilities_inquiry                  3   0.4%  \n",
      "  excess_baggage_fee                          3   0.4%  \n",
      "  contact_number_update_request               2   0.3%  \n",
      "  flight_schedule_change_inquiry              2   0.3%  \n",
      "  seat_selection_inquiry                      2   0.3%  \n",
      "  unaccompanied_minor_service_inquiry         2   0.3%  \n",
      "  refund_request_dispute                      2   0.3%  \n",
      "  flight_rebooking_request                    2   0.3%  \n",
      "  flight_disruption_notification              2   0.3%  \n",
      "\n",
      "   Unique source articles covered : 83\n",
      "   Unique issue categories        : 409\n",
      "\n",
      "ğŸ“Š Resolution Quality:\n",
      "   Avg length  : 494 chars\n",
      "   Min length  : 301 chars\n",
      "   Max length  : 1,038 chars\n",
      "\n",
      "ğŸ“Š Query Length:\n",
      "   Avg length  : 104 chars\n",
      "   Min length  : 40 chars\n",
      "   Max length  : 247 chars\n",
      "\n",
      "ğŸ“ TRAIN Sample:\n",
      "   ticket_id      : CP-2026-TR-0001\n",
      "   issue_category : contact_number_update_request\n",
      "   source_article : Updating Your Guest or Contact Details\n",
      "   customer_query : Hello, I'm trying to update my contact details but I'm having some trouble. I've booked a flight with Cebu Pacific and would like to change my contact number. What's the best way to do this?\n",
      "   resolution     : You can update your contact number via Manage Booking. Please follow these steps: 1. Log in to your account, 2. Select your booking, 3. Go to the contact information section and update your phone numb...\n",
      "\n",
      "ğŸ“ VAL Sample:\n",
      "   ticket_id      : CP-2026-VL-0001\n",
      "   issue_category : payment_dispute\n",
      "   source_article : Payment and Receipt Concerns\n",
      "   customer_query : I would like to pay for my booking using my visa card. However, I've been charged an additional fee. Can I dispute this charge?\n",
      "   resolution     : Unfortunately, Cebu Pacific does not support other payment methods other than those listed in our 'Accepted Forms of Payment' section. However, if you have any concerns regarding your payment, you may...\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ Files ready:\n",
      "   cebu_pacific_trainset_v2.jsonl\n",
      "   cebu_pacific_valset_v2.jsonl\n",
      "   ../Customer_Ticket_Support_Agent/cebu_pacific_trainset.jsonl  â† optimization pipeline\n",
      "   ../Customer_Ticket_Support_Agent/cebu_pacific_valset.jsonl    â† optimization pipeline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 8: Verification â€” Comprehensive Stats + Samples\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from collections import Counter\n",
    "\n",
    "all_data = trainset_v2 + valset_v2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹ ENRICHED DATASET v2 SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n   Total examples : {len(all_data):,}\")\n",
    "print(f\"   Train          : {len(trainset_v2):,}\")\n",
    "print(f\"   Val            : {len(valset_v2):,}\")\n",
    "\n",
    "# â”€â”€ Issue category distribution (top 25) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cat_counts = Counter(ex[\"issue_category\"] for ex in all_data)\n",
    "print(f\"\\n{'Issue Category':<40} {'Count':>6}  {'%':>5}\")\n",
    "print(\"-\" * 55)\n",
    "for cat, n in sorted(cat_counts.items(), key=lambda x: -x[1])[:25]:\n",
    "    bar = \"â–ˆ\" * int(n / len(all_data) * 200)\n",
    "    print(f\"  {cat:<38} {n:>6}  {n/len(all_data)*100:>4.1f}%  {bar}\")\n",
    "\n",
    "# â”€â”€ Source article coverage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sources = Counter(ex[\"source_article\"] for ex in all_data)\n",
    "print(f\"\\n   Unique source articles covered : {len(sources)}\")\n",
    "print(f\"   Unique issue categories        : {len(cat_counts)}\")\n",
    "\n",
    "# â”€â”€ Resolution quality â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "lengths = [len(ex[\"resolution\"]) for ex in all_data]\n",
    "q_lengths = [len(ex[\"customer_query\"]) for ex in all_data]\n",
    "print(f\"\\nğŸ“Š Resolution Quality:\")\n",
    "print(f\"   Avg length  : {sum(lengths) // len(lengths):,} chars\")\n",
    "print(f\"   Min length  : {min(lengths):,} chars\")\n",
    "print(f\"   Max length  : {max(lengths):,} chars\")\n",
    "print(f\"\\nğŸ“Š Query Length:\")\n",
    "print(f\"   Avg length  : {sum(q_lengths) // len(q_lengths):,} chars\")\n",
    "print(f\"   Min length  : {min(q_lengths):,} chars\")\n",
    "print(f\"   Max length  : {max(q_lengths):,} chars\")\n",
    "\n",
    "# â”€â”€ Sample from each set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for label, dataset in [(\"TRAIN\", trainset_v2), (\"VAL\", valset_v2)]:\n",
    "    sample = dataset[0]\n",
    "    print(f\"\\nğŸ“ {label} Sample:\")\n",
    "    print(f\"   ticket_id      : {sample['ticket_id']}\")\n",
    "    print(f\"   issue_category : {sample['issue_category']}\")\n",
    "    print(f\"   source_article : {sample['source_article']}\")\n",
    "    print(f\"   customer_query : {sample['customer_query']}\")\n",
    "    print(f\"   resolution     : {sample['resolution'][:200].replace(chr(10), ' ')}...\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ¯ Files ready:\")\n",
    "print(f\"   {TRAINSET_OUT}\")\n",
    "print(f\"   {VALSET_OUT}\")\n",
    "if OPT_DIR.exists():\n",
    "    print(f\"   {OPT_DIR}/cebu_pacific_trainset.jsonl  â† optimization pipeline\")\n",
    "    print(f\"   {OPT_DIR}/cebu_pacific_valset.jsonl    â† optimization pipeline\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
