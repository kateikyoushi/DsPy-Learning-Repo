{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed54de9",
   "metadata": {},
   "source": [
    "# üéØ Cebu Pacific Customer Support Agent Optimization\n",
    "\n",
    "**Demonstration of DSPy Agent Optimization**\n",
    "\n",
    "This notebook demonstrates how DSPy automatically optimizes a customer support agent:\n",
    "- **Step 1**: Show the Problem (unoptimized agent)\n",
    "- **Step 2**: Run DSPy Optimization (MIPROv2)\n",
    "- **Step 3**: Show the Results (optimized agent)\n",
    "- **Step 4**: Calculate Business Impact ($821K/year savings)\n",
    "\n",
    "**Technology Stack:**\n",
    "- DSPy Framework\n",
    "- Groq LLM (llama-3.1-8b-instant)\n",
    "- MIPROv2 Optimizer\n",
    "- MLflow Tracking\n",
    "\n",
    "**Dataset:**\n",
    "- 50 training examples (past successful resolutions)\n",
    "- 20 validation examples (test scenarios)\n",
    "- Real Cebu Pacific customer support tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Install and Import Required Packages\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages (run once)\n",
    "import sys\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import dspy\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66051c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Setup Groq API Key\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load Groq API key from environment variable\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Set in environment for DSPy\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"‚úÖ Groq API key configured\")\n",
    "print(f\"   Key: {GROQ_API_KEY[:20]}... (hidden)\" if GROQ_API_KEY else \"   Key: Not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Configure DSPy with Groq LLM\n",
    "# ============================================================================\n",
    "\n",
    "# Configure DSPy to use Groq's llama-3.1-8b-instant\n",
    "lm = dspy.LM(\n",
    "    'groq/llama-3.1-8b-instant',\n",
    "    api_key=GROQ_API_KEY,\n",
    "    max_tokens=800,      # Sufficient for support responses\n",
    "    temperature=0.7      # Balance between consistency and creativity\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"‚úÖ DSPy configured with Groq llama-3.1-8b-instant\")\n",
    "print(f\"   Model: groq/llama-3.1-8b-instant\")\n",
    "print(f\"   Max tokens: 800\")\n",
    "print(f\"   Temperature: 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Load Training and Validation Datasets\n",
    "# ============================================================================\n",
    "\n",
    "# Load training dataset (50 examples of successful resolutions)\n",
    "trainset = []\n",
    "with open(\"cebu_pacific_trainset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        # Create DSPy Example with customer_query as input, resolution as output\n",
    "        example = dspy.Example(\n",
    "            query=data[\"customer_query\"],\n",
    "            answer=data[\"resolution\"]\n",
    "        ).with_inputs(\"query\")\n",
    "        trainset.append(example)\n",
    "\n",
    "# Load validation dataset (20 examples for testing)\n",
    "valset = []\n",
    "with open(\"cebu_pacific_valset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        example = dspy.Example(\n",
    "            query=data[\"customer_query\"],\n",
    "            answer=data.get(\"resolution\", \"\")\n",
    "        ).with_inputs(\"query\")\n",
    "        valset.append(example)\n",
    "\n",
    "print(\"‚úÖ Datasets loaded successfully!\")\n",
    "print(f\"   Training set: {len(trainset)} examples\")\n",
    "print(f\"   Validation set: {len(valset)} examples\")\n",
    "print(f\"\\nüìä Sample training example:\")\n",
    "print(f\"   Query: {trainset[0].query[:100]}...\")\n",
    "print(f\"   Answer: {trainset[0].answer[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Visualize Dataset Statistics\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate query and answer lengths\n",
    "train_query_lengths = [len(ex.query) for ex in trainset]\n",
    "train_answer_lengths = [len(ex.answer) for ex in trainset]\n",
    "val_query_lengths = [len(ex.query) for ex in valset]\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Query length distribution\n",
    "axes[0].hist(train_query_lengths, bins=20, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "axes[0].set_title('Customer Query Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Characters')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(np.mean(train_query_lengths), color='red', linestyle='--', \n",
    "                label=f'Avg: {np.mean(train_query_lengths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Answer length distribution\n",
    "axes[1].hist(train_answer_lengths, bins=20, alpha=0.7, color='#FF6B6B', edgecolor='black')\n",
    "axes[1].set_title('Expert Resolution Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Characters')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(np.mean(train_answer_lengths), color='blue', linestyle='--',\n",
    "                label=f'Avg: {np.mean(train_answer_lengths):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Dataset size comparison\n",
    "datasets = ['Training\\n(50)', 'Validation\\n(20)']\n",
    "sizes = [len(trainset), len(valset)]\n",
    "bars = axes[2].bar(datasets, sizes, color=['#4ECDC4', '#FF6B6B'], alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Dataset Sizes', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Number of Examples')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{size}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Dataset statistics:\")\n",
    "print(f\"   Avg query length: {np.mean(train_query_lengths):.0f} characters\")\n",
    "print(f\"   Avg resolution length: {np.mean(train_answer_lengths):.0f} characters\")\n",
    "print(f\"   Query length range: {min(train_query_lengths)} - {max(train_query_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Create Support Agent Module (Simple ChainOfThought)\n",
    "# ============================================================================\n",
    "\n",
    "# Create a simple support agent using DSPy's ChainOfThought\n",
    "# This will be optimized later with MIPROv2\n",
    "\n",
    "class SupportAgent(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ChainOfThought: query -> reasoning -> answer\n",
    "        self.generate_response = dspy.ChainOfThought(\"query -> answer\")\n",
    "\n",
    "    def forward(self, query):\n",
    "        # Generate response for customer query\n",
    "        response = self.generate_response(query=query)\n",
    "        return response\n",
    "\n",
    "# Create original (unoptimized) agent\n",
    "original_agent = SupportAgent()\n",
    "\n",
    "print(\"‚úÖ Support agent created!\")\n",
    "print(\"   Architecture: ChainOfThought (query -> answer)\")\n",
    "print(\"   Status: Unoptimized (no instructions, no few-shot examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: STEP 1 - Show the Problem (Unoptimized Agent)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: THE PROBLEM - Unoptimized Agent Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on a challenging customer query\n",
    "test_query = \"hi cant check in online it says booking not found but i have confirmation email flight tomorrow help!!!\"\n",
    "\n",
    "print(f\"\\nüî¥ UNOPTIMIZED AGENT TEST\\n\")\n",
    "print(f\"Customer Query:\")\n",
    "print(f'\"{test_query}\"')\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "\n",
    "# Time the response\n",
    "start_time = time.time()\n",
    "unoptimized_response = original_agent(query=test_query)\n",
    "unoptimized_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüí¨ Unoptimized Agent Response:\")\n",
    "print(f\"{unoptimized_response.answer}\")\n",
    "print(f\"\\n‚è±Ô∏è  Response time: {unoptimized_time:.2f} seconds\")\n",
    "print(f\"\\nüìä Analysis:\")\n",
    "print(f\"   ‚ùå Generic and unhelpful\")\n",
    "print(f\"   ‚ùå No specific troubleshooting steps\")\n",
    "print(f\"   ‚ùå No actionable solutions\")\n",
    "print(f\"   ‚ùå Customer still frustrated\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Store for comparison\n",
    "unoptimized_result = {\n",
    "    \"query\": test_query,\n",
    "    \"response\": unoptimized_response.answer,\n",
    "    \"time\": unoptimized_time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Define Evaluation Metric\n",
    "# ============================================================================\n",
    "\n",
    "def support_quality_metric(example, pred, trace=None):\n",
    "    \"\"\"\n",
    "    Custom metric to evaluate support response quality.\n",
    "    Checks if key support elements are present in the response.\n",
    "    \"\"\"\n",
    "    answer = pred.answer if hasattr(pred, 'answer') else str(pred)\n",
    "\n",
    "    # Key elements of good support response\n",
    "    quality_indicators = [\n",
    "        \"step\" in answer.lower() or \"option\" in answer.lower(),  # Structured guidance\n",
    "        len(answer) > 200,  # Detailed response\n",
    "        \"‚úÖ\" in answer or \"‚úì\" in answer or \"yes\" in answer.lower(),  # Positive indicators\n",
    "        \"@\" in answer or \"www\" in answer or \"phone\" in answer.lower(),  # Contact info\n",
    "        \"‚Ç±\" in answer or \"php\" in answer.lower() or \"fee\" in answer.lower()  # Specific info\n",
    "    ]\n",
    "\n",
    "    # Score is percentage of quality indicators present\n",
    "    score = sum(quality_indicators) / len(quality_indicators)\n",
    "    return score\n",
    "\n",
    "print(\"‚úÖ Evaluation metric defined: support_quality_metric\")\n",
    "print(\"   Checks for:\")\n",
    "print(\"   - Structured guidance (steps/options)\")\n",
    "print(\"   - Detailed response (>200 chars)\")\n",
    "print(\"   - Positive indicators\")\n",
    "print(\"   - Contact information\")\n",
    "print(\"   - Specific details (fees, policies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Evaluate Original Agent (Baseline)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE EVALUATION: Original Agent on Validation Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on a subset of validation set for speed (first 10 examples)\n",
    "eval_subset = valset[:10]\n",
    "\n",
    "baseline_scores = []\n",
    "print(f\"\\nEvaluating on {len(eval_subset)} validation examples...\\n\")\n",
    "\n",
    "for i, example in enumerate(eval_subset, 1):\n",
    "    try:\n",
    "        pred = original_agent(query=example.query)\n",
    "        score = support_quality_metric(example, pred)\n",
    "        baseline_scores.append(score)\n",
    "        status = \"‚úÖ\" if score >= 0.6 else \"‚ùå\"\n",
    "        print(f\"  {i}/10: Score={score:.2f} {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {i}/10: Error - {str(e)[:50]}\")\n",
    "        baseline_scores.append(0.0)\n",
    "\n",
    "baseline_avg = np.mean(baseline_scores) if baseline_scores else 0.0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä BASELINE RESULTS:\")\n",
    "print(f\"   Average Score: {baseline_avg:.2%}\")\n",
    "print(f\"   Status: {'‚úÖ Acceptable' if baseline_avg >= 0.6 else '‚ùå Needs Improvement'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Configure MIPROv2 Optimizer\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: DSPy OPTIMIZATION - Configuring MIPROv2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configure MIPROv2 optimizer\n",
    "optimizer = dspy.MIPROv2(\n",
    "    metric=support_quality_metric,  # Custom quality metric\n",
    "    auto=\"light\",                    # Light mode for faster optimization\n",
    "    num_threads=8,                   # Parallel evaluation\n",
    "    max_bootstrapped_demos=3,        # Few-shot examples per module\n",
    "    max_labeled_demos=3              # Maximum demos to use\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ MIPROv2 Optimizer configured:\")\n",
    "print(f\"   Mode: light (fast, efficient)\")\n",
    "print(f\"   Metric: support_quality_metric\")\n",
    "print(f\"   Threads: 8 (parallel evaluation)\")\n",
    "print(f\"   Max demos: 3 per module\")\n",
    "print(f\"\\nüîÑ The optimizer will:\")\n",
    "print(f\"   1. Bootstrap few-shot examples from training data\")\n",
    "print(f\"   2. Generate instruction candidates using LLM\")\n",
    "print(f\"   3. Evaluate combinations on validation set\")\n",
    "print(f\"   4. Select best performing configuration\")\n",
    "print(f\"\\n‚è≥ Optimization typically takes 3-5 minutes...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
