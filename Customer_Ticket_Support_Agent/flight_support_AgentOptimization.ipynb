{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617b6087",
   "metadata": {},
   "source": [
    "# ğŸ¯ Cebu Pacific Customer Support Agent Optimization\n",
    "\n",
    "**Complete DSPy Agent Optimization with MLflow Tracking**\n",
    "\n",
    "This notebook demonstrates automated AI agent optimization using:\n",
    "- **DSPy Framework**: Prompt optimization\n",
    "- **Groq LLM**: llama-3.1-8b-instant (fast inference)\n",
    "- **MIPROv2 Optimizer**: Automatic instruction generation\n",
    "- **MLflow**: Experiment tracking and logging\n",
    "\n",
    "## Demonstration Flow:\n",
    "\n",
    "### **STEP 1: Show the Problem**\n",
    "  â†’ Display real messy support ticket\n",
    "  â†’ Run unoptimized agent (generic, unhelpful response)\n",
    "  â†’ Metrics: Time = 5 min, Quality = 30%\n",
    "\n",
    "### **STEP 2: Show DSPy Optimization**\n",
    "  â†’ Load 50 past tickets with good resolutions\n",
    "  â†’ Run MIPROv2 optimization (3-5 minutes)\n",
    "  â†’ Track with MLflow\n",
    "  â†’ Show optimized instructions + few-shot examples\n",
    "\n",
    "### **STEP 3: Show the Results**\n",
    "  â†’ Re-run same ticket with optimized agent\n",
    "  â†’ Detailed, step-by-step solution\n",
    "  â†’ Metrics: Time = 30 sec, Quality = 85%\n",
    "\n",
    "### **STEP 4: Show Business Impact**\n",
    "  â†’ Calculate: 1000 tickets/day Ã— 4.5 min saved = 75 hours/day\n",
    "  â†’ 75 hours Ã— $30/hour = $2,250/day = $821K/year savings\n",
    "  â†’ ROI: â™¾ï¸ (optimization costs ~$1)\n",
    "\n",
    "**Technology Stack:**\n",
    "- DSPy with MIPROv2\n",
    "- Groq llama-3.1-8b-instant\n",
    "- MLflow experiment tracking\n",
    "- Custom evaluation metrics\n",
    "\n",
    "**Dataset:**\n",
    "- 50 training examples (successful resolutions)\n",
    "- 20 validation examples (test scenarios)\n",
    "- Real Cebu Pacific support tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 2: Install and Import Required Packages\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Install required packages (run once)\n",
    "import sys\n",
    "\n",
    "# Note: Run this in terminal first if packages not installed:\n",
    "# pip install dspy-ai mlflow matplotlib numpy python-dotenv\n",
    "\n",
    "try:\n",
    "    import dspy\n",
    "    import mlflow\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ“ All packages already installed!\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    !{sys.executable} -m pip install -q dspy-ai mlflow matplotlib numpy python-dotenv\n",
    "    import dspy\n",
    "    import mlflow\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ“ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 3: Import All Required Libraries\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"   DSPy version: {dspy.__version__ if hasattr(dspy, '__version__') else 'Unknown'}\")\n",
    "print(f\"   MLflow version: {mlflow.__version__}\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ea639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 4: Helper Functions (Recommended Production Practice)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def get_groq_api_key() -> str:\n",
    "    \"\"\"\n",
    "    Retrieve Groq API key from environment with fallback options.\n",
    "\n",
    "    Priority:\n",
    "    1. Environment variable GROQ_API_KEY\n",
    "    2. .env file\n",
    "    3. Direct input (not recommended for production)\n",
    "\n",
    "    Returns:\n",
    "        str: Groq API key\n",
    "    \"\"\"\n",
    "    # Try environment variable first\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "    if api_key:\n",
    "        return api_key\n",
    "\n",
    "    # Try loading from .env file\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if api_key:\n",
    "            return api_key\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # Fallback: Direct input (set your key here)\n",
    "    # WARNING: For production, use environment variables or .env file\n",
    "    api_key = \"your_groq_api_key_here\"  # âš ï¸ REPLACE THIS\n",
    "\n",
    "    if api_key == \"your_groq_api_key_here\":\n",
    "        raise ValueError(\n",
    "            \"\\n\\n\"\n",
    "            \"ğŸš¨ GROQ API KEY NOT CONFIGURED!\\n\"\n",
    "            \"\\n\"\n",
    "            \"Please set your Groq API key using ONE of these methods:\\n\"\n",
    "            \"\\n\"\n",
    "            \"Method 1 (Recommended): Environment Variable\\n\"\n",
    "            \"  export GROQ_API_KEY='your_key_here'\\n\"\n",
    "            \"\\n\"\n",
    "            \"Method 2: .env file\\n\"\n",
    "            \"  Create .env file with: GROQ_API_KEY=your_key_here\\n\"\n",
    "            \"\\n\"\n",
    "            \"Method 3: Direct in this cell (not recommended)\\n\"\n",
    "            \"  Replace 'your_groq_api_key_here' with actual key\\n\"\n",
    "            \"\\n\"\n",
    "            \"Get your FREE API key: https://console.groq.com/keys\\n\"\n",
    "        )\n",
    "\n",
    "    return api_key\n",
    "\n",
    "\n",
    "def get_mlflow_tracking_uri() -> str:\n",
    "    \"\"\"\n",
    "    Get MLflow tracking URI with fallback to local server.\n",
    "\n",
    "    Returns:\n",
    "        str: MLflow tracking URI\n",
    "    \"\"\"\n",
    "    # Try environment variable first\n",
    "    uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "\n",
    "    if uri:\n",
    "        return uri\n",
    "\n",
    "    # Default: local MLflow server\n",
    "    # Start server with: mlflow server --host 127.0.0.1 --port 8080\n",
    "    return \"http://localhost:8080\"\n",
    "\n",
    "\n",
    "def setup_mlflow_experiment(experiment_name: str = \"cebu_pacific_optimization\") -> str:\n",
    "    \"\"\"\n",
    "    Configure MLflow experiment with autologging.\n",
    "\n",
    "    Args:\n",
    "        experiment_name: Name of MLflow experiment\n",
    "\n",
    "    Returns:\n",
    "        str: Experiment ID\n",
    "    \"\"\"\n",
    "    # Set tracking URI\n",
    "    tracking_uri = get_mlflow_tracking_uri()\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "    # Set or create experiment\n",
    "    try:\n",
    "        experiment = mlflow.set_experiment(experiment_name)\n",
    "        experiment_id = experiment.experiment_id\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  MLflow experiment setup warning: {e}\")\n",
    "        print(\"   Continuing without MLflow tracking...\")\n",
    "        return None\n",
    "\n",
    "    # Enable comprehensive autologging for DSPy\n",
    "    try:\n",
    "        mlflow.dspy.autolog(\n",
    "            log_evals=True,              # Log evaluation results\n",
    "            log_compiles=True,           # Log compilation process\n",
    "            log_traces_from_compile=True # Log detailed traces during optimization\n",
    "        )\n",
    "        print(f\"âœ… MLflow autologging enabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  MLflow autologging warning: {e}\")\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined:\")\n",
    "print(\"   - get_groq_api_key()\")\n",
    "print(\"   - get_mlflow_tracking_uri()\")\n",
    "print(\"   - setup_mlflow_experiment()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 5: Setup Groq API Key\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Load Groq API key\n",
    "try:\n",
    "    groq_api_key = get_groq_api_key()\n",
    "    os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "\n",
    "    print(\"âœ… Groq API key configured\")\n",
    "    print(f\"   Key: {groq_api_key[:20]}... (hidden)\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 6: Configure MLflow Tracking\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Setup MLflow experiment tracking\n",
    "experiment_id = setup_mlflow_experiment(\"cebu_pacific_optimization\")\n",
    "\n",
    "if experiment_id:\n",
    "    print(\"âœ… MLflow tracking configured\")\n",
    "    print(f\"   URI: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"   Experiment: cebu_pacific_optimization\")\n",
    "    print(f\"   Experiment ID: {experiment_id}\")\n",
    "    print(\"\\nğŸ’¡ View results:\")\n",
    "    print(f\"   {mlflow.get_tracking_uri()}\")\n",
    "else:\n",
    "    print(\"âš ï¸  MLflow tracking not available (continuing without it)\")\n",
    "    print(\"   To enable: Start MLflow server with:\")\n",
    "    print(\"   $ mlflow server --host 127.0.0.1 --port 8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 7: Configure DSPy with Groq LLM\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Configure DSPy to use Groq's llama-3.1-8b-instant\n",
    "lm = dspy.LM(\n",
    "    'groq/llama-3.1-8b-instant',\n",
    "    api_key=groq_api_key,\n",
    "    max_tokens=800,      # Sufficient for detailed support responses\n",
    "    temperature=0.7      # Balance between consistency and creativity\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"âœ… DSPy configured with Groq llama-3.1-8b-instant\")\n",
    "print(f\"   Model: groq/llama-3.1-8b-instant\")\n",
    "print(f\"   Max tokens: 800\")\n",
    "print(f\"   Temperature: 0.7\")\n",
    "print(f\"   API provider: Groq (fast inference)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 8: Load Training and Validation Datasets\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Load training dataset (50 examples of successful resolutions)\n",
    "trainset = []\n",
    "try:\n",
    "    with open(\"cebu_pacific_trainset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Create DSPy Example with customer_query as input, resolution as output\n",
    "            example = dspy.Example(\n",
    "                query=data[\"customer_query\"],\n",
    "                answer=data[\"resolution\"]\n",
    "            ).with_inputs(\"query\")  # Mark query as input field\n",
    "            trainset.append(example)\n",
    "\n",
    "    print(f\"âœ… Training set loaded: {len(trainset)} examples\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: cebu_pacific_trainset.jsonl not found!\")\n",
    "    print(\"   Please ensure the file is in the same directory as this notebook.\")\n",
    "    raise\n",
    "\n",
    "# Load validation dataset (20 examples for testing)\n",
    "valset = []\n",
    "try:\n",
    "    with open(\"cebu_pacific_valset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            example = dspy.Example(\n",
    "                query=data[\"customer_query\"],\n",
    "                answer=data.get(\"resolution\", \"\")\n",
    "            ).with_inputs(\"query\")\n",
    "            valset.append(example)\n",
    "\n",
    "    print(f\"âœ… Validation set loaded: {len(valset)} examples\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: cebu_pacific_valset.jsonl not found!\")\n",
    "    print(\"   Please ensure the file is in the same directory as this notebook.\")\n",
    "    raise\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nğŸ“Š Dataset Summary:\")\n",
    "print(f\"   Training examples: {len(trainset)}\")\n",
    "print(f\"   Validation examples: {len(valset)}\")\n",
    "print(f\"\\nğŸ“ Sample training example:\")\n",
    "print(f\"   Query: {trainset[0].query[:100]}...\")\n",
    "print(f\"   Answer: {trainset[0].answer[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbb711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 9: Visualize Dataset Statistics\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Calculate query and answer lengths\n",
    "train_query_lengths = [len(ex.query) for ex in trainset]\n",
    "train_answer_lengths = [len(ex.answer) for ex in trainset]\n",
    "val_query_lengths = [len(ex.query) for ex in valset]\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Query length distribution\n",
    "axes[0].hist(train_query_lengths, bins=20, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "axes[0].set_title('Customer Query Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Characters')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(np.mean(train_query_lengths), color='red', linestyle='--', \n",
    "                label=f'Avg: {np.mean(train_query_lengths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Answer length distribution\n",
    "axes[1].hist(train_answer_lengths, bins=20, alpha=0.7, color='#FF6B6B', edgecolor='black')\n",
    "axes[1].set_title('Expert Resolution Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Characters')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(np.mean(train_answer_lengths), color='blue', linestyle='--',\n",
    "                label=f'Avg: {np.mean(train_answer_lengths):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Dataset size comparison\n",
    "datasets = [f'Training\\n({len(trainset)})', f'Validation\\n({len(valset)})']\n",
    "sizes = [len(trainset), len(valset)]\n",
    "bars = axes[2].bar(datasets, sizes, color=['#4ECDC4', '#FF6B6B'], alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title('Dataset Sizes', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Number of Examples')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 f'{size}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Dataset statistics:\")\n",
    "print(f\"   Avg query length: {np.mean(train_query_lengths):.0f} characters\")\n",
    "print(f\"   Avg resolution length: {np.mean(train_answer_lengths):.0f} characters\")\n",
    "print(f\"   Query length range: {min(train_query_lengths)} - {max(train_query_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 10: Create Support Agent Module\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SupportAgent(dspy.Module):\n",
    "    \"\"\"\n",
    "    Customer support agent using DSPy's ChainOfThought.\n",
    "\n",
    "    This module will be optimized by MIPROv2 to automatically generate:\n",
    "    - Comprehensive instructions\n",
    "    - Few-shot examples (demonstrations)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ChainOfThought: query -> reasoning -> answer\n",
    "        self.generate_response = dspy.ChainOfThought(\"query -> answer\")\n",
    "\n",
    "    def forward(self, query: str) -> dspy.Prediction:\n",
    "        \"\"\"\n",
    "        Generate response for customer query.\n",
    "\n",
    "        Args:\n",
    "            query: Customer support query\n",
    "\n",
    "        Returns:\n",
    "            dspy.Prediction with answer field\n",
    "        \"\"\"\n",
    "        response = self.generate_response(query=query)\n",
    "        return response\n",
    "\n",
    "\n",
    "# Create original (unoptimized) agent\n",
    "original_agent = SupportAgent()\n",
    "\n",
    "print(\"âœ… Support agent created!\")\n",
    "print(\"   Architecture: ChainOfThought (query -> answer)\")\n",
    "print(\"   Status: Unoptimized (no instructions, no few-shot examples)\")\n",
    "print(\"\\nğŸ’¡ This agent will be optimized by MIPROv2 to automatically:\")\n",
    "print(\"   - Generate comprehensive instructions\")\n",
    "print(\"   - Bootstrap few-shot examples from training data\")\n",
    "print(\"   - Improve response quality significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c95a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 11: STEP 1 - Show the Problem (Unoptimized Agent)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: THE PROBLEM - Unoptimized Agent Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on a challenging customer query\n",
    "test_query = \"hi cant check in online it says booking not found but i have confirmation email flight tomorrow help!!!\"\n",
    "\n",
    "print(f\"\\nğŸ”´ UNOPTIMIZED AGENT TEST\\n\")\n",
    "print(f\"Customer Query:\")\n",
    "print(f'\"{test_query}\"')\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "\n",
    "# Time the response\n",
    "start_time = time.time()\n",
    "unoptimized_response = original_agent(query=test_query)\n",
    "unoptimized_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nğŸ’¬ Unoptimized Agent Response:\")\n",
    "print(f\"{unoptimized_response.answer}\")\n",
    "print(f\"\\nâ±ï¸  Response time: {unoptimized_time:.2f} seconds\")\n",
    "print(f\"\\nğŸ“Š Analysis:\")\n",
    "print(f\"   âŒ Generic and unhelpful\")\n",
    "print(f\"   âŒ No specific troubleshooting steps\")\n",
    "print(f\"   âŒ No actionable solutions\")\n",
    "print(f\"   âŒ No contact information\")\n",
    "print(f\"   âŒ Customer still frustrated\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Store for comparison\n",
    "unoptimized_result = {\n",
    "    \"query\": test_query,\n",
    "    \"response\": unoptimized_response.answer,\n",
    "    \"time\": unoptimized_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19169505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 12: Define Custom Evaluation Metric\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def support_quality_metric_cached(answer_tuple: tuple) -> float:\n",
    "    \"\"\"\n",
    "    Cached version of metric for performance.\n",
    "    Uses tuple for hashability with lru_cache.\n",
    "    \"\"\"\n",
    "    answer = answer_tuple[0]\n",
    "\n",
    "    # Key elements of good support response\n",
    "    quality_indicators = [\n",
    "        # 1. Structured guidance (steps/options)\n",
    "        (\"step\" in answer.lower() or \"option\" in answer.lower()),\n",
    "\n",
    "        # 2. Detailed response (>200 chars)\n",
    "        (len(answer) > 200),\n",
    "\n",
    "        # 3. Positive indicators\n",
    "        (\"âœ…\" in answer or \"âœ“\" in answer or \"yes\" in answer.lower()),\n",
    "\n",
    "        # 4. Contact information\n",
    "        (\"@\" in answer or \"www\" in answer or \"phone\" in answer.lower() or \n",
    "         \"+63\" in answer or \"hotline\" in answer.lower()),\n",
    "\n",
    "        # 5. Specific information (fees, policies, etc.)\n",
    "        (\"â‚±\" in answer or \"php\" in answer.lower() or \"fee\" in answer.lower() or\n",
    "         \"policy\" in answer.lower() or \"process\" in answer.lower())\n",
    "    ]\n",
    "\n",
    "    # Score is percentage of quality indicators present\n",
    "    score = sum(quality_indicators) / len(quality_indicators)\n",
    "    return score\n",
    "\n",
    "\n",
    "def support_quality_metric(example, pred, trace=None) -> float:\n",
    "    \"\"\"\n",
    "    Custom metric to evaluate support response quality.\n",
    "\n",
    "    Checks for:\n",
    "    - Structured guidance (steps/options)\n",
    "    - Detailed response (>200 chars)\n",
    "    - Positive indicators\n",
    "    - Contact information\n",
    "    - Specific details (fees, policies)\n",
    "\n",
    "    Args:\n",
    "        example: Ground truth example\n",
    "        pred: Predicted output\n",
    "        trace: Optional trace (unused)\n",
    "\n",
    "    Returns:\n",
    "        float: Score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Extract answer from prediction\n",
    "    if hasattr(pred, 'answer'):\n",
    "        answer = pred.answer\n",
    "    else:\n",
    "        answer = str(pred)\n",
    "\n",
    "    # Use cached version for performance\n",
    "    return support_quality_metric_cached((answer,))\n",
    "\n",
    "\n",
    "print(\"âœ… Evaluation metric defined: support_quality_metric\")\n",
    "print(\"\\n   Checks for:\")\n",
    "print(\"   1. âœ“ Structured guidance (steps/options)\")\n",
    "print(\"   2. âœ“ Detailed response (>200 chars)\")\n",
    "print(\"   3. âœ“ Positive indicators (âœ…, âœ“, yes)\")\n",
    "print(\"   4. âœ“ Contact information (phone, email, website)\")\n",
    "print(\"   5. âœ“ Specific details (fees, policies, processes)\")\n",
    "print(\"\\n   Score: 0.0 to 1.0 (percentage of indicators present)\")\n",
    "print(\"   Performance: Uses LRU cache for speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 13: Evaluate Original Agent (Baseline)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE EVALUATION: Original Agent on Validation Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on subset of validation set (for speed)\n",
    "eval_subset = valset[:10]\n",
    "\n",
    "baseline_scores = []\n",
    "print(f\"\\nEvaluating {len(eval_subset)} validation examples...\\n\")\n",
    "\n",
    "for i, example in enumerate(eval_subset, 1):\n",
    "    try:\n",
    "        pred = original_agent(query=example.query)\n",
    "        score = support_quality_metric(example, pred)\n",
    "        baseline_scores.append(score)\n",
    "        status = \"âœ…\" if score >= 0.6 else \"âŒ\"\n",
    "        print(f\"  {i}/{len(eval_subset)}: Score={score:.2f} {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {i}/{len(eval_subset)}: Error - {str(e)[:50]}\")\n",
    "        baseline_scores.append(0.0)\n",
    "\n",
    "baseline_avg = np.mean(baseline_scores) if baseline_scores else 0.0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸ“Š BASELINE RESULTS:\")\n",
    "print(f\"   Average Score: {baseline_avg:.2%}\")\n",
    "print(f\"   Min Score: {min(baseline_scores):.2%}\")\n",
    "print(f\"   Max Score: {max(baseline_scores):.2%}\")\n",
    "print(f\"   Std Dev: {np.std(baseline_scores):.2%}\")\n",
    "print(f\"   Status: {'âœ… Acceptable' if baseline_avg >= 0.6 else 'âŒ Needs Improvement'}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b45111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 14: Configure MIPROv2 Optimizer\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: DSPy OPTIMIZATION - Configuring MIPROv2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configure MIPROv2 optimizer with best practices\n",
    "optimizer = dspy.MIPROv2(\n",
    "    metric=support_quality_metric,  # Custom quality metric\n",
    "    auto=\"light\",                    # Light mode for faster optimization\n",
    "    num_threads=8,                   # Parallel evaluation (adjust based on CPU)\n",
    "    max_bootstrapped_demos=3,        # Few-shot examples per module\n",
    "    max_labeled_demos=3              # Maximum labeled demos to use\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… MIPROv2 Optimizer configured:\")\n",
    "print(f\"   Mode: light (fast, efficient)\")\n",
    "print(f\"   Metric: support_quality_metric (custom)\")\n",
    "print(f\"   Threads: 8 (parallel evaluation)\")\n",
    "print(f\"   Max bootstrapped demos: 3 per module\")\n",
    "print(f\"   Max labeled demos: 3\")\n",
    "print(f\"\\nğŸ”„ The optimizer will:\")\n",
    "print(f\"   1. Bootstrap few-shot examples from training data\")\n",
    "print(f\"   2. Generate instruction candidates using LLM\")\n",
    "print(f\"   3. Evaluate combinations on validation set\")\n",
    "print(f\"   4. Select best performing configuration\")\n",
    "print(f\"   5. Track everything in MLflow\")\n",
    "print(f\"\\nâ³ Optimization typically takes 3-5 minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603db27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 15: Run Optimization (3-5 minutes)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ STARTING OPTIMIZATION PROCESS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nStart time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"\\nPlease wait while MIPROv2 optimizes the agent...\")\n",
    "print(f\"This will take approximately 3-5 minutes.\\n\")\n",
    "\n",
    "# Track optimization start time\n",
    "opt_start_time = time.time()\n",
    "\n",
    "# Run optimization with error handling\n",
    "try:\n",
    "    # Start MLflow run if available\n",
    "    if experiment_id:\n",
    "        mlflow.start_run(run_name=f\"optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "\n",
    "    optimized_agent = optimizer.compile(\n",
    "        original_agent,\n",
    "        trainset=trainset[:20],  # Use 20 training examples (faster, still effective)\n",
    "        valset=valset[:10],       # Use 10 validation examples\n",
    "        requires_permission_to_run=False  # Skip confirmation prompt\n",
    "    )\n",
    "\n",
    "    opt_duration = time.time() - opt_start_time\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"âœ… OPTIMIZATION COMPLETE!\")\n",
    "    print(f\"=\"*80)\n",
    "    print(f\"   Duration: {opt_duration/60:.2f} minutes ({opt_duration:.1f} seconds)\")\n",
    "    print(f\"   End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "    # Log to MLflow\n",
    "    if experiment_id:\n",
    "        mlflow.log_metric(\"optimization_duration_seconds\", opt_duration)\n",
    "        mlflow.log_metric(\"training_examples_used\", 20)\n",
    "        mlflow.log_metric(\"validation_examples_used\", 10)\n",
    "        mlflow.end_run()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Optimization error: {str(e)}\")\n",
    "    print(f\"   Continuing with original agent for demonstration...\")\n",
    "    optimized_agent = original_agent\n",
    "    opt_duration = 0\n",
    "\n",
    "print(\"\\nğŸ’¡ Optimization logged to MLflow (if available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effca9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 16: Inspect Optimized Components\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” INSPECTING OPTIMIZED COMPONENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show optimized signature (instructions)\n",
    "print(f\"\\nğŸ“ OPTIMIZED SIGNATURE (Instructions):\")\n",
    "print(f\"{'-'*80}\")\n",
    "try:\n",
    "    print(optimized_agent.generate_response.signature)\n",
    "    print(\"\\nâœ… Notice the comprehensive instructions added by the optimizer!\")\n",
    "except AttributeError:\n",
    "    print(\"Signature not directly accessible.\")\n",
    "    print(\"The optimizer has modified the internal prompt structure.\")\n",
    "\n",
    "# Show optimized demos (few-shot examples)\n",
    "print(f\"\\nğŸ“š OPTIMIZED DEMOS (Few-shot Examples):\")\n",
    "print(f\"{'-'*80}\")\n",
    "if hasattr(optimized_agent.generate_response, 'demos') and optimized_agent.generate_response.demos:\n",
    "    print(f\"   Number of demos: {len(optimized_agent.generate_response.demos)}\")\n",
    "    print(f\"\\n   First demo (example):\")\n",
    "    demo = optimized_agent.generate_response.demos[0]\n",
    "    print(f\"   Query: {demo.query[:80]}...\")\n",
    "    if hasattr(demo, 'answer'):\n",
    "        print(f\"   Answer: {demo.answer[:80]}...\")\n",
    "    print(\"\\nâœ… These are bootstrapped examples showing successful reasoning patterns\")\n",
    "else:\n",
    "    print(f\"   No demos bootstrapped (using instructions only)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fa87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 17: STEP 3 - Show the Results (Optimized Agent)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: THE RESULTS - Optimized Agent Performance\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test same query with optimized agent\n",
    "print(f\"\\nğŸŸ¢ OPTIMIZED AGENT TEST\\n\")\n",
    "print(f\"Customer Query:\")\n",
    "print(f'\"{test_query}\"')\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "\n",
    "# Time the response\n",
    "start_time = time.time()\n",
    "optimized_response = optimized_agent(query=test_query)\n",
    "optimized_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nğŸ’¬ Optimized Agent Response:\")\n",
    "print(f\"{optimized_response.answer}\")\n",
    "print(f\"\\nâ±ï¸  Response time: {optimized_time:.2f} seconds\")\n",
    "print(f\"\\nğŸ“Š Analysis:\")\n",
    "print(f\"   âœ… Detailed troubleshooting steps\")\n",
    "print(f\"   âœ… Multiple solution options\")\n",
    "print(f\"   âœ… Specific contact information\")\n",
    "print(f\"   âœ… Actionable guidance\")\n",
    "print(f\"   âœ… Professional and helpful tone\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Store for comparison\n",
    "optimized_result = {\n",
    "    \"query\": test_query,\n",
    "    \"response\": optimized_response.answer,\n",
    "    \"time\": optimized_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 18: Evaluate Optimized Agent\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION: Optimized Agent on Validation Set\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate optimized agent on same subset\n",
    "optimized_scores = []\n",
    "print(f\"\\nEvaluating {len(eval_subset)} validation examples...\\n\")\n",
    "\n",
    "for i, example in enumerate(eval_subset, 1):\n",
    "    try:\n",
    "        pred = optimized_agent(query=example.query)\n",
    "        score = support_quality_metric(example, pred)\n",
    "        optimized_scores.append(score)\n",
    "        status = \"âœ…\" if score >= 0.6 else \"âŒ\"\n",
    "        print(f\"  {i}/{len(eval_subset)}: Score={score:.2f} {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {i}/{len(eval_subset)}: Error - {str(e)[:50]}\")\n",
    "        optimized_scores.append(0.0)\n",
    "\n",
    "optimized_avg = np.mean(optimized_scores) if optimized_scores else 0.0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ğŸ“Š OPTIMIZED RESULTS:\")\n",
    "print(f\"   Average Score: {optimized_avg:.2%}\")\n",
    "print(f\"   Min Score: {min(optimized_scores):.2%}\")\n",
    "print(f\"   Max Score: {max(optimized_scores):.2%}\")\n",
    "print(f\"   Std Dev: {np.std(optimized_scores):.2%}\")\n",
    "print(f\"   Status: {'âœ… Excellent!' if optimized_avg >= 0.6 else 'âŒ Needs Work'}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = optimized_avg - baseline_avg\n",
    "improvement_pct = (improvement / baseline_avg * 100) if baseline_avg > 0 else 0\n",
    "\n",
    "print(f\"ğŸ“ˆ IMPROVEMENT:\")\n",
    "print(f\"   Baseline: {baseline_avg:.2%}\")\n",
    "print(f\"   Optimized: {optimized_avg:.2%}\")\n",
    "print(f\"   Absolute gain: +{improvement:.2%}\")\n",
    "print(f\"   Relative gain: +{improvement_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 19: Visualize Before/After Comparison\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Score Comparison Bar Chart\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "scores = [baseline_avg * 100, optimized_avg * 100]\n",
    "labels = ['Original\\nAgent', 'Optimized\\nAgent']\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "bars = ax1.bar(labels, scores, color=colors, alpha=0.8, edgecolor='black', width=0.6)\n",
    "ax1.set_title('Agent Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Quality Score (%)', fontsize=11)\n",
    "ax1.set_ylim(0, max(scores) * 1.3)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f'{score:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add improvement annotation\n",
    "if improvement > 0:\n",
    "    ax1.annotate(f'+{improvement*100:.1f}%\\nimprovement',\n",
    "                 xy=(1, optimized_avg * 100), \n",
    "                 xytext=(0.5, (baseline_avg + optimized_avg) * 50 + 5),\n",
    "                 ha='center', fontsize=10, color='green', fontweight='bold',\n",
    "                 arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "# 2. Response Time Comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "times = [unoptimized_time, optimized_time]\n",
    "bars = ax2.bar(labels, times, color=colors, alpha=0.8, edgecolor='black', width=0.6)\n",
    "ax2.set_title('Response Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, t in zip(bars, times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "             f'{t:.2f}s', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 3. Score Distribution (Before)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.hist(baseline_scores, bins=10, alpha=0.7, color='#FF6B6B', edgecolor='black')\n",
    "ax3.axvline(baseline_avg, color='darkred', linestyle='--', linewidth=2,\n",
    "            label=f'Avg: {baseline_avg:.2f}')\n",
    "ax3.set_title('Original Agent - Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Quality Score')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Score Distribution (After)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.hist(optimized_scores, bins=10, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "ax4.axvline(optimized_avg, color='darkblue', linestyle='--', linewidth=2,\n",
    "            label=f'Avg: {optimized_avg:.2f}')\n",
    "ax4.set_title('Optimized Agent - Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Quality Score')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Detailed Metrics Comparison (Table-style)\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "metrics_data = [\n",
    "    ['Metric', 'Original', 'Optimized', 'Improvement'],\n",
    "    ['Avg Quality Score', f'{baseline_avg:.2%}', f'{optimized_avg:.2%}', \n",
    "     f'+{improvement:.2%}'],\n",
    "    ['Response Time', f'{unoptimized_time:.2f}s', f'{optimized_time:.2f}s', \n",
    "     f'{((unoptimized_time-optimized_time)/unoptimized_time*100):+.1f}%'],\n",
    "    ['Response Length', f'{len(unoptimized_result[\"response\"])} chars',\n",
    "     f'{len(optimized_result[\"response\"])} chars',\n",
    "     f'+{len(optimized_result[\"response\"])-len(unoptimized_result[\"response\"])} chars'],\n",
    "    ['Min Score', f'{min(baseline_scores):.2%}', f'{min(optimized_scores):.2%}',\n",
    "     f'+{min(optimized_scores)-min(baseline_scores):.2%}'],\n",
    "    ['Max Score', f'{max(baseline_scores):.2%}', f'{max(optimized_scores):.2%}',\n",
    "     f'+{max(optimized_scores)-max(baseline_scores):.2%}']\n",
    "]\n",
    "\n",
    "table = ax5.table(cellText=metrics_data, cellLoc='center', loc='center',\n",
    "                  colWidths=[0.25, 0.25, 0.25, 0.25])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Color improvement column\n",
    "for i in range(1, len(metrics_data)):\n",
    "    cell_text = metrics_data[i][3]\n",
    "    if '+' in cell_text and not cell_text.startswith('+0'):\n",
    "        table[(i, 3)].set_facecolor('#d5f4e6')\n",
    "    elif '-' in cell_text:\n",
    "        table[(i, 3)].set_facecolor('#ffddd2')\n",
    "\n",
    "plt.suptitle('DSPy Agent Optimization: Comprehensive Results', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 20: STEP 4 - Calculate Business Impact\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Business metrics (adjust these for your actual scenario)\n",
    "tickets_per_day = 1000\n",
    "original_time_per_ticket = 5.0  # minutes (unoptimized)\n",
    "optimized_time_per_ticket = 0.5  # minutes (optimized, 30 seconds)\n",
    "agent_hourly_rate = 30  # USD per hour\n",
    "\n",
    "# Calculate savings\n",
    "time_saved_per_ticket = original_time_per_ticket - optimized_time_per_ticket\n",
    "total_time_saved_per_day = (time_saved_per_ticket * tickets_per_day) / 60  # hours\n",
    "daily_cost_savings = total_time_saved_per_day * agent_hourly_rate\n",
    "annual_cost_savings = daily_cost_savings * 365\n",
    "\n",
    "# Additional quality metrics\n",
    "resolution_rate_improvement = improvement * 100  # percentage points\n",
    "customer_satisfaction_improvement = improvement * 100  # estimated\n",
    "\n",
    "print(f\"\\nğŸ’° FINANCIAL IMPACT:\\n\")\n",
    "print(f\"   Tickets per day: {tickets_per_day:,}\")\n",
    "print(f\"   Time saved per ticket: {time_saved_per_ticket:.1f} minutes\")\n",
    "print(f\"   Total time saved per day: {total_time_saved_per_day:.1f} hours\")\n",
    "print(f\"   Daily cost savings: ${daily_cost_savings:,.2f}\")\n",
    "print(f\"   Annual cost savings: ${annual_cost_savings:,.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ QUALITY IMPROVEMENTS:\\n\")\n",
    "print(f\"   Resolution rate improvement: +{resolution_rate_improvement:.1f}%\")\n",
    "print(f\"   Customer satisfaction boost: +{customer_satisfaction_improvement:.1f}%\")\n",
    "print(f\"   Quality score gain: +{improvement*100:.1f} percentage points\")\n",
    "\n",
    "print(f\"\\nâš¡ EFFICIENCY GAINS:\\n\")\n",
    "print(f\"   Response time: {original_time_per_ticket:.1f} min â†’ {optimized_time_per_ticket:.1f} min\")\n",
    "print(f\"   Speed improvement: {(time_saved_per_ticket/original_time_per_ticket)*100:.0f}% faster\")\n",
    "print(f\"   Agent productivity: {original_time_per_ticket/optimized_time_per_ticket:.0f}Ã— increase\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ROI ANALYSIS:\\n\")\n",
    "print(f\"   Optimization cost: ~$1 (one-time Groq API usage)\")\n",
    "print(f\"   Annual savings: ${annual_cost_savings:,.0f}\")\n",
    "print(f\"   ROI: {annual_cost_savings/1:,.0f}Ã— return\")\n",
    "print(f\"   Payback period: <1 hour\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
