{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a977930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test cell (run before Cell 1)\n",
    "import os\n",
    "from helper import get_groq_api_key\n",
    "\n",
    "api_key = get_groq_api_key()\n",
    "print(f\"âœ… GROQ API Key: {api_key[:10]}...\" if api_key else \"âŒ Missing GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test cell (run before Cell 1)\n",
    "try:\n",
    "    import requests\n",
    "    import dspy\n",
    "    import mlflow\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ… All packages installed correctly!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Missing package: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test cell (run after Cell 6)\n",
    "test_results = search_wikipedia(\"Python programming language\")\n",
    "print(f\"âœ… Wikipedia search working! Got {len(test_results)} results\")\n",
    "print(f\"Sample: {test_results[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9a905",
   "metadata": {},
   "source": [
    "# L4: Optimize DSPy Agent with DSPy Optimizer\n",
    "\n",
    "â³ **Note (Kernel Starting)**: This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.\n",
    "\n",
    "**Updated for Groq llama-3.1-8b-instant** - Conservative token usage and latest DSPy documentation\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’» **Access files**: Click \"File\" â†’ \"Open\" to view `requirements.txt` and `helper.py`\n",
    "\n",
    "â¬‡ **Download Notebook**: Click \"File\" â†’ \"Download as\" â†’ \"Notebook (.ipynb)\"\n",
    "\n",
    "ðŸ“’ **For more help**, please see the \"Appendix â€“ Tips, Help, and Download\" Lesson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup Groq API Key\n",
    "from helper import get_groq_api_key\n",
    "import os\n",
    "\n",
    "# Load Groq API key from environment\n",
    "groq_api_key = get_groq_api_key()\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "\n",
    "print(\"âœ“ Groq API key loaded and set in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0be383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configure MLflow Tracking\n",
    "import mlflow\n",
    "from helper import get_mlflow_tracking_uri\n",
    "\n",
    "# Set up MLflow tracking server\n",
    "mlflow_tracking_uri = get_mlflow_tracking_uri()\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"dspy_course_4\")\n",
    "\n",
    "# Enable autologging with full optimization tracking\n",
    "mlflow.dspy.autolog(\n",
    "    log_evals=True,           # Log evaluation results\n",
    "    log_compiles=True,        # Log compilation process\n",
    "    log_traces_from_compile=True  # Log detailed traces during optimization\n",
    ")\n",
    "\n",
    "print(\"âœ“ MLflow tracking configured\")\n",
    "print(f\"  URI: {mlflow_tracking_uri}\")\n",
    "print(f\"  Experiment: dspy_course_4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f74873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configure DSPy with Groq LLM\n",
    "import dspy\n",
    "\n",
    "# Configure DSPy to use Groq's llama-3.1-8b-instant\n",
    "lm = dspy.LM(\n",
    "    'groq/llama-3.1-8b-instant',\n",
    "    api_key=groq_api_key,\n",
    "    max_tokens=512,      # Conservative token usage for RAG answers\n",
    "    temperature=0.7      # Balance creativity and factual accuracy\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"âœ“ DSPy configured with Groq llama-3.1-8b-instant\")\n",
    "print(f\"  Model: groq/llama-3.1-8b-instant\")\n",
    "print(f\"  Max tokens: 512 (conservative)\")\n",
    "print(f\"  Temperature: 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.5: Install Visualization Libraries\n",
    "# Install matplotlib for visualizations (if not already installed)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ“ matplotlib already available\")\n",
    "except ImportError:\n",
    "    print(\"Installing matplotlib...\")\n",
    "    %pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ“ matplotlib installed and imported\")\n",
    "\n",
    "# Configure matplotlib for better notebook display\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ Visualization setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd45a",
   "metadata": {},
   "source": [
    "## Build a RAG Agent\n",
    "\n",
    "We'll create a Wikipedia-based Retrieval-Augmented Generation (RAG) agent using DSPy's ReAct module.\n",
    "\n",
    "**ReAct** = Reasoning + Acting - The agent decides when to search for more information before answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define Wikipedia API Search Tool (FIXED - Added User-Agent)\n",
    "import requests\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "def search_wikipedia(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search Wikipedia using official API (replaces ColBERT due to server timeout).\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        \n",
    "    Returns:\n",
    "        List of Wikipedia article extracts (top 3)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Wikipedia API endpoint\n",
    "        api_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        \n",
    "        # REQUIRED: User-Agent header (Wikipedia API policy)\n",
    "        headers = {\n",
    "            'User-Agent': 'DSPyLearningBot/1.0 (Educational Project; Python/requests)'\n",
    "        }\n",
    "        \n",
    "        # First, search for relevant articles\n",
    "        search_params = {\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'list': 'search',\n",
    "            'srsearch': query,\n",
    "            'srlimit': 3,\n",
    "            'utf8': 1\n",
    "        }\n",
    "        \n",
    "        search_response = requests.get(\n",
    "            api_url, \n",
    "            params=search_params, \n",
    "            headers=headers,  # Added headers\n",
    "            timeout=10\n",
    "        )\n",
    "        search_response.raise_for_status()  # Raise error for bad status codes\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Get full extracts for each search result\n",
    "        search_results = search_data.get('query', {}).get('search', [])\n",
    "        \n",
    "        if not search_results:\n",
    "            return [f\"No Wikipedia results found for: {query}\"]\n",
    "        \n",
    "        for item in search_results:\n",
    "            page_id = item.get('pageid')\n",
    "            \n",
    "            if not page_id:\n",
    "                continue\n",
    "            \n",
    "            # Fetch article extract\n",
    "            extract_params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'pageids': page_id,\n",
    "                'prop': 'extracts',\n",
    "                'exintro': True,  # Just the intro section\n",
    "                'explaintext': True,  # Plain text, no HTML\n",
    "                'utf8': 1\n",
    "            }\n",
    "            \n",
    "            extract_response = requests.get(\n",
    "                api_url, \n",
    "                params=extract_params, \n",
    "                headers=headers,  # Added headers\n",
    "                timeout=10\n",
    "            )\n",
    "            extract_response.raise_for_status()\n",
    "            extract_data = extract_response.json()\n",
    "            \n",
    "            # Extract the text\n",
    "            pages = extract_data.get('query', {}).get('pages', {})\n",
    "            if pages:\n",
    "                page_content = list(pages.values())[0]\n",
    "                extract = page_content.get('extract', '')\n",
    "                \n",
    "                if extract:\n",
    "                    # Limit to ~500 characters to mimic ColBERT chunks\n",
    "                    extract_chunk = extract[:500] + \"...\" if len(extract) > 500 else extract\n",
    "                    results.append(extract_chunk)\n",
    "            \n",
    "            # Small delay to be respectful to Wikipedia API\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # Return results or fallback message\n",
    "        if results:\n",
    "            return results\n",
    "        else:\n",
    "            return [f\"No content found for: {query}\"]\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return [f\"Wikipedia search timed out for: {query}\"]\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Wikipedia API request error: {e}\")\n",
    "        return [f\"Search error: Unable to retrieve Wikipedia data for: {query}\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Wikipedia API error: {e}\")\n",
    "        return [f\"Search error: {str(e)}\"]\n",
    "\n",
    "print(\"âœ“ Wikipedia API search tool defined\")\n",
    "print(\"  Data source: Wikipedia official API (en.wikipedia.org)\")\n",
    "print(\"  User-Agent: DSPyLearningBot/1.0 (required by Wikipedia)\")\n",
    "print(\"  Returns: Top 3 article extracts per query (~500 chars each)\")\n",
    "print(\"  Status: âœ… Ready for optimization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create ReAct Agent\n",
    "react = dspy.ReAct(\"question -> answer\", tools=[search_wikipedia])\n",
    "\n",
    "print(\"âœ“ ReAct agent created\")\n",
    "print(\"  Input: question\")\n",
    "print(\"  Output: answer\")\n",
    "print(\"  Tools: [search_wikipedia]\")\n",
    "print(\"  Agent will autonomously decide when to search for information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell: Verify Wikipedia API Search (Optional - can delete after testing)\n",
    "print(\"Testing Wikipedia API search...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_query = \"Eiffel Tower Paris France\"\n",
    "test_results = search_wikipedia(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Results: {len(test_results)} chunks returned\")\n",
    "print(\"\\n--- Sample Result ---\")\n",
    "print(test_results[0][:200] + \"...\" if len(test_results[0]) > 200 else test_results[0])\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… Wikipedia API working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdade",
   "metadata": {},
   "source": [
    "## Load Training and Validation Datasets\n",
    "\n",
    "We'll use a subset of the **HotpotQA** dataset - a question-answering benchmark based on Wikipedia data.\n",
    "\n",
    "- **Training set**: Used to bootstrap few-shot examples\n",
    "- **Validation set**: Used to evaluate candidate programs during optimization\n",
    "- **Dataset size**: Can be as small as 20 records (unlike traditional ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Load Training Dataset\n",
    "import json\n",
    "\n",
    "# Load training set\n",
    "trainset = []\n",
    "with open(\"trainset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        example = dspy.Example(**json.loads(line))\n",
    "        trainset.append(example.with_inputs(\"question\"))\n",
    "\n",
    "print(f\"âœ“ Training set loaded: {len(trainset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Load Validation Dataset\n",
    "\n",
    "# Load validation set\n",
    "valset = []\n",
    "with open(\"valset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        example = dspy.Example(**json.loads(line))\n",
    "        valset.append(example.with_inputs(\"question\"))\n",
    "\n",
    "print(f\"âœ“ Validation set loaded: {len(valset)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283410c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Inspect Dataset Sample\n",
    "\n",
    "# Overview of the dataset structure\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE TRAINING EXAMPLE:\")\n",
    "print(\"=\" * 70)\n",
    "print(trainset[0])\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDataset structure:\")\n",
    "print(\"  - 'question': Input field\")\n",
    "print(\"  - 'answer': Expected output (ground truth)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f899d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11.5: Visualize Dataset Statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate question lengths\n",
    "train_lengths = [len(ex.question) for ex in trainset]\n",
    "val_lengths = [len(ex.question) for ex in valset]\n",
    "\n",
    "# Plot histograms of question lengths\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_lengths, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Question Length Distribution - Training Set')\n",
    "plt.xlabel('Question Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(val_lengths, bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.title('Question Length Distribution - Validation Set')\n",
    "plt.xlabel('Question Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot dataset sizes comparison\n",
    "plt.figure(figsize=(6, 4))\n",
    "datasets = ['Training', 'Validation']\n",
    "sizes = [len(trainset), len(valset)]\n",
    "bars = plt.bar(datasets, sizes, color=['blue', 'orange'], alpha=0.7, edgecolor='black')\n",
    "plt.title('Dataset Sizes Comparison')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{size}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Dataset visualizations generated\")\n",
    "print(f\"  Training set: {len(trainset)} examples, avg question length: {sum(train_lengths)/len(train_lengths):.1f} chars\")\n",
    "print(f\"  Validation set: {len(valset)} examples, avg question length: {sum(val_lengths)/len(val_lengths):.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b664a4",
   "metadata": {},
   "source": [
    "## Configure MIPROv2 Optimizer\n",
    "\n",
    "**MIPROv2** (Multi-prompt Instruction Proposal Optimizer v2) automatically optimizes:\n",
    "\n",
    "1. **Prompt Templates** - Generates comprehensive instructions for each module\n",
    "2. **Few-Shot Examples** - Bootstraps high-quality demonstrations from training data\n",
    "\n",
    "### How it works:\n",
    "1. **Bootstrap** examples by running training data through the program\n",
    "2. **Generate** instruction candidates using the LLM\n",
    "3. **Sample** combinations of instructions + few-shot examples\n",
    "4. **Evaluate** each candidate program on validation set\n",
    "5. **Select** the best performing configuration\n",
    "\n",
    "**Auto mode options:**\n",
    "- `light`: Fast optimization (recommended for getting started)\n",
    "- `medium`: Balanced optimization\n",
    "- `heavy`: Thorough optimization (more time/cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af349115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Setup MIPROv2 Optimizer\n",
    "tp = dspy.MIPROv2(\n",
    "    metric=dspy.evaluate.answer_exact_match,  # How to score program outputs\n",
    "    auto=\"light\",      # Conservative optimization mode\n",
    "    num_threads=16     # Parallel evaluation threads\n",
    ")\n",
    "\n",
    "print(\"âœ“ MIPROv2 Optimizer configured\")\n",
    "print(\"  Metric: answer_exact_match\")\n",
    "print(\"  Mode: light (fast, conservative)\")\n",
    "print(\"  Threads: 16 (parallel evaluation)\")\n",
    "print(\"\\nThe optimizer will:\")\n",
    "print(\"  1. Bootstrap few-shot examples from training data\")\n",
    "print(\"  2. Generate instruction candidates via LLM\")\n",
    "print(\"  3. Evaluate combinations on validation set\")\n",
    "print(\"  4. Select best performing configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c80cf",
   "metadata": {},
   "source": [
    "## Load Cache and Run Optimization\n",
    "\n",
    "â±ï¸ **Note**: We're using a pre-computed cache (`memory_cache.pkl`) to speed up the process for this demo.\n",
    "\n",
    "In production, you can run without cache - the optimizer will make real LLM calls.\n",
    "\n",
    "ðŸ”„ **Optimization Process** (typically 5-15 minutes without cache):\n",
    "- Tests multiple candidate programs\n",
    "- Continuously tracks and improves scores\n",
    "- Uses statistical sampling to find optimal configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Load Memory Cache\n",
    "# dspy.cache.load_memory_cache(\"./memory_cache.pkl\")  # Commented out - cache API may have changed in DSPy 2.6\n",
    "\n",
    "print(\"âœ“ Memory cache loading skipped (cache API not available in current DSPy version)\")\n",
    "print(\"  In production: Check DSPy documentation for current cache loading method\")\n",
    "print(\"  Proceeding with fresh optimization (will make real LLM calls)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Compile Optimized Program\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING OPTIMIZATION PROCESS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This will:\")\n",
    "print(\"  - Bootstrap few-shot examples\")\n",
    "print(\"  - Generate instruction candidates\")\n",
    "print(\"  - Evaluate candidate programs\")\n",
    "print(\"  - Track results in MLflow\")\n",
    "print(\"\\nðŸ”„ Compiling...\\n\")\n",
    "\n",
    "optimized_react = tp.compile(\n",
    "    react,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    "    requires_permission_to_run=False,  # Skip confirmation prompt\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ“ OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae997061",
   "metadata": {},
   "source": [
    "## Inspect Optimized Components\n",
    "\n",
    "Let's examine what the optimizer changed in our ReAct agent:\n",
    "\n",
    "### Original Program:\n",
    "- **Signature**: Simple `question -> answer` with no instructions\n",
    "- **Demos**: No few-shot examples\n",
    "\n",
    "### Optimized Program:\n",
    "- **Signature**: Enhanced with comprehensive instructions\n",
    "- **Demos**: Bootstrapped few-shot examples showing good reasoning traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: View Optimized Signature\n",
    "print(\"=\" * 70)\n",
    "print(\"ORIGINAL SIGNATURE:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"question -> answer\")\n",
    "print(\"(No instructions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZED SIGNATURE:\")\n",
    "print(\"=\" * 70)\n",
    "print(optimized_react.react.signature)\n",
    "print(\"\\nâœ“ Notice the comprehensive instructions added by the optimizer!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09014ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: View Optimized Demos\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZED FEW-SHOT EXAMPLES (Demos):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nNumber of demos: {len(optimized_react.react.demos)}\")\n",
    "\n",
    "if optimized_react.react.demos:\n",
    "    print(\"\\n--- First Demo ---\")\n",
    "    print(optimized_react.react.demos[0])\n",
    "    print(\"\\nâœ“ These are bootstrapped examples showing successful reasoning traces\")\n",
    "else:\n",
    "    print(\"No demos found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03779882",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "\n",
    "Now let's compare the **original** vs **optimized** agent performance on the validation set.\n",
    "\n",
    "**Metric**: Exact Match - The answer must match the ground truth exactly.\n",
    "\n",
    "We'll see how much improvement the optimizer achieved without any manual prompt engineering!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Setup Evaluator\n",
    "evaluator = dspy.Evaluate(\n",
    "    metric=dspy.evaluate.answer_exact_match,  # Exact match scoring\n",
    "    devset=valset,          # Evaluation dataset\n",
    "    display_table=True,     # Show results table\n",
    "    display_progress=True,  # Show progress bar\n",
    "    num_threads=24,         # Parallel evaluation\n",
    ")\n",
    "\n",
    "print(\"âœ“ Evaluator configured\")\n",
    "print(f\"  Metric: answer_exact_match\")\n",
    "print(f\"  Evaluation set: {len(valset)} examples\")\n",
    "print(f\"  Threads: 24\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Evaluate Original Agent\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING ORIGINAL REACT AGENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Running evaluation on validation set...\")\n",
    "print(\"(This shows baseline performance without optimization)\\n\")\n",
    "\n",
    "original_score = evaluator(react)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ðŸ“Š ORIGINAL SCORE: {original_score:.4f} ({original_score*100:.2f}%)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Evaluate Optimized Agent\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING OPTIMIZED REACT AGENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Running evaluation on validation set...\")\n",
    "print(\"(This shows performance after MIPROv2 optimization)\\n\")\n",
    "\n",
    "optimized_score = evaluator(optimized_react)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ðŸ“Š OPTIMIZED SCORE: {optimized_score:.4f} ({optimized_score*100:.2f}%)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Performance Comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Metric':<20} {'Original':<15} {'Optimized':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "original_pct = original_score * 100\n",
    "optimized_pct = optimized_score * 100\n",
    "improvement = optimized_pct - original_pct\n",
    "relative_gain = (improvement / original_pct * 100) if original_pct > 0 else 0\n",
    "\n",
    "print(f\"{'Exact Match Score':<20} {original_pct:>6.2f}%{'':<8} {optimized_pct:>6.2f}%{'':<8} +{improvement:>5.2f}%\")\n",
    "print(f\"{'Relative Gain':<20} {'':<15} {'':<15} {relative_gain:>6.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ¨ ACHIEVEMENT UNLOCKED: Automatic Prompt Engineering!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWithout ANY manual prompt engineering, the MIPROv2 optimizer:\")\n",
    "print(f\"  âœ“ Generated optimized instructions automatically\")\n",
    "print(f\"  âœ“ Bootstrapped high-quality few-shot examples\")\n",
    "print(f\"  âœ“ Improved performance by {improvement:.2f} percentage points\")\n",
    "print(f\"  âœ“ Achieved {relative_gain:.1f}% relative improvement\")\n",
    "print(\"\\nThis is the power of DSPy Optimization!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69081996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24.5: Visualize Performance Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for comparison\n",
    "scores = [original_score * 100, optimized_score * 100]\n",
    "labels = ['Original Agent', 'Optimized Agent']\n",
    "colors = ['#FF6B6B', '#4ECDC4']  # Red for original, teal for optimized\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(labels, scores, color=colors, alpha=0.8, edgecolor='black', width=0.6)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('DSPy Agent Performance Comparison\\n(Exact Match Score)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.ylim(0, max(scores) * 1.2)  # Add some headroom\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{score:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Add improvement annotation\n",
    "improvement = optimized_score * 100 - original_score * 100\n",
    "if improvement > 0:\n",
    "    plt.annotate(f'Improvement: +{improvement:.2f}%', \n",
    "                 xy=(1, optimized_score * 100), \n",
    "                 xytext=(0.5, (original_score + optimized_score)/2 * 100 + 5),\n",
    "                 ha='center', fontsize=10, color='green',\n",
    "                 arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional visualization: Performance gauge-style chart\n",
    "fig, ax = plt.subplots(figsize=(8, 2), subplot_kw=dict(projection='polar'))\n",
    "ax.set_thetamin(0)\n",
    "ax.set_thetamax(180)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "# Plot arcs for scores\n",
    "theta_original = original_score * 180\n",
    "theta_optimized = optimized_score * 180\n",
    "\n",
    "ax.barh(0.5, theta_original/180, left=0, height=0.3, color='#FF6B6B', alpha=0.7)\n",
    "ax.barh(0.5, theta_optimized/180, left=0, height=0.3, color='#4ECDC4', alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "ax.text(0, 0.7, '0%', ha='center', va='bottom', fontsize=10)\n",
    "ax.text(180, 0.7, '100%', ha='center', va='bottom', fontsize=10)\n",
    "ax.text(theta_original, 0.3, f'Original\\n{original_score*100:.1f}%', ha='center', va='top', fontsize=9, color='white', fontweight='bold')\n",
    "ax.text(theta_optimized, 0.3, f'Optimized\\n{optimized_score*100:.1f}%', ha='center', va='top', fontsize=9, color='white', fontweight='bold')\n",
    "\n",
    "ax.set_title('Performance Gauge: Before vs After Optimization', fontsize=12, fontweight='bold', pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Performance comparison visualizations generated\")\n",
    "print(f\"  Original Score: {original_score*100:.2f}%\")\n",
    "print(f\"  Optimized Score: {optimized_score*100:.2f}%\")\n",
    "print(f\"  Absolute Improvement: +{improvement:.2f} percentage points\")\n",
    "print(f\"  Relative Improvement: +{relative_gain:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
