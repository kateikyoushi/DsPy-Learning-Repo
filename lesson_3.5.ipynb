{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c9a905",
   "metadata": {},
   "source": [
    "# L4: Optimize DSPy Agent with DSPy Optimizer\n",
    "\n",
    "â³ **Note (Kernel Starting)**: This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.\n",
    "\n",
    "**Updated for Groq llama-3.1-8b-instant** - Conservative token usage and latest DSPy documentation\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’» **Access files**: Click \"File\" â†’ \"Open\" to view `requirements.txt` and `helper.py`\n",
    "\n",
    "â¬‡ **Download Notebook**: Click \"File\" â†’ \"Download as\" â†’ \"Notebook (.ipynb)\"\n",
    "\n",
    "ðŸ“’ **For more help**, please see the \"Appendix â€“ Tips, Help, and Download\" Lesson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup Groq API Key\n",
    "from helper import get_groq_api_key\n",
    "import os\n",
    "\n",
    "# Load Groq API key from environment\n",
    "groq_api_key = get_groq_api_key()\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "\n",
    "print(\"âœ“ Groq API key loaded and set in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0be383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configure MLflow Tracking\n",
    "import mlflow\n",
    "from helper import get_mlflow_tracking_uri\n",
    "\n",
    "# Set up MLflow tracking server\n",
    "mlflow_tracking_uri = get_mlflow_tracking_uri()\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"dspy_course_4\")\n",
    "\n",
    "# Enable autologging with full optimization tracking\n",
    "mlflow.dspy.autolog(\n",
    "    log_evals=True,           # Log evaluation results\n",
    "    log_compiles=True,        # Log compilation process\n",
    "    log_traces_from_compile=True  # Log detailed traces during optimization\n",
    ")\n",
    "\n",
    "print(\"âœ“ MLflow tracking configured\")\n",
    "print(f\"  URI: {mlflow_tracking_uri}\")\n",
    "print(f\"  Experiment: dspy_course_4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f74873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configure DSPy with Groq LLM\n",
    "import dspy\n",
    "\n",
    "# Configure DSPy to use Groq's llama-3.1-8b-instant\n",
    "lm = dspy.LM(\n",
    "    'groq/llama-3.1-8b-instant',\n",
    "    api_key=groq_api_key,\n",
    "    max_tokens=512,      # Conservative token usage for RAG answers\n",
    "    temperature=0.7      # Balance creativity and factual accuracy\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"âœ“ DSPy configured with Groq llama-3.1-8b-instant\")\n",
    "print(f\"  Model: groq/llama-3.1-8b-instant\")\n",
    "print(f\"  Max tokens: 512 (conservative)\")\n",
    "print(f\"  Temperature: 0.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd45a",
   "metadata": {},
   "source": [
    "## Build a RAG Agent\n",
    "\n",
    "We'll create a Wikipedia-based Retrieval-Augmented Generation (RAG) agent using DSPy's ReAct module.\n",
    "\n",
    "**ReAct** = Reasoning + Acting - The agent decides when to search for more information before answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define Wikipedia Search Tool\n",
    "def search_wikipedia(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Search Wikipedia abstracts and return top 3 relevant text chunks.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks from Wikipedia\n",
    "    \"\"\"\n",
    "    results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
    "    return [x[\"text\"] for x in results]\n",
    "\n",
    "print(\"âœ“ Wikipedia search tool defined\")\n",
    "print(\"  Data source: Wikipedia abstracts (ColBERTv2)\")\n",
    "print(\"  Returns: Top 3 relevant chunks per query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create ReAct Agent\n",
    "react = dspy.ReAct(\"question -> answer\", tools=[search_wikipedia])\n",
    "\n",
    "print(\"âœ“ ReAct agent created\")\n",
    "print(\"  Input: question\")\n",
    "print(\"  Output: answer\")\n",
    "print(\"  Tools: [search_wikipedia]\")\n",
    "print(\"  Agent will autonomously decide when to search for information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdade",
   "metadata": {},
   "source": [
    "## Load Training and Validation Datasets\n",
    "\n",
    "We'll use a subset of the **HotpotQA** dataset - a question-answering benchmark based on Wikipedia data.\n",
    "\n",
    "- **Training set**: Used to bootstrap few-shot examples\n",
    "- **Validation set**: Used to evaluate candidate programs during optimization\n",
    "- **Dataset size**: Can be as small as 20 records (unlike traditional ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Load Training Dataset\n",
    "import json\n",
    "\n",
    "# Load training set\n",
    "trainset = []\n",
    "with open(\"trainset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        example = dspy.Example(**json.loads(line))\n",
    "        trainset.append(example.with_inputs(\"question\"))\n",
    "\n",
    "print(f\"âœ“ Training set loaded: {len(trainset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Load Validation Dataset\n",
    "\n",
    "# Load validation set\n",
    "valset = []\n",
    "with open(\"valset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        example = dspy.Example(**json.loads(line))\n",
    "        valset.append(example.with_inputs(\"question\"))\n",
    "\n",
    "print(f\"âœ“ Validation set loaded: {len(valset)} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283410c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Inspect Dataset Sample\n",
    "\n",
    "# Overview of the dataset structure\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE TRAINING EXAMPLE:\")\n",
    "print(\"=\" * 70)\n",
    "print(trainset[0])\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDataset structure:\")\n",
    "print(\"  - 'question': Input field\")\n",
    "print(\"  - 'answer': Expected output (ground truth)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b664a4",
   "metadata": {},
   "source": [
    "## Configure MIPROv2 Optimizer\n",
    "\n",
    "**MIPROv2** (Multi-prompt Instruction Proposal Optimizer v2) automatically optimizes:\n",
    "\n",
    "1. **Prompt Templates** - Generates comprehensive instructions for each module\n",
    "2. **Few-Shot Examples** - Bootstraps high-quality demonstrations from training data\n",
    "\n",
    "### How it works:\n",
    "1. **Bootstrap** examples by running training data through the program\n",
    "2. **Generate** instruction candidates using the LLM\n",
    "3. **Sample** combinations of instructions + few-shot examples\n",
    "4. **Evaluate** each candidate program on validation set\n",
    "5. **Select** the best performing configuration\n",
    "\n",
    "**Auto mode options:**\n",
    "- `light`: Fast optimization (recommended for getting started)\n",
    "- `medium`: Balanced optimization\n",
    "- `heavy`: Thorough optimization (more time/cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af349115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Setup MIPROv2 Optimizer\n",
    "tp = dspy.MIPROv2(\n",
    "    metric=dspy.evaluate.answer_exact_match,  # How to score program outputs\n",
    "    auto=\"light\",      # Conservative optimization mode\n",
    "    num_threads=16     # Parallel evaluation threads\n",
    ")\n",
    "\n",
    "print(\"âœ“ MIPROv2 Optimizer configured\")\n",
    "print(\"  Metric: answer_exact_match\")\n",
    "print(\"  Mode: light (fast, conservative)\")\n",
    "print(\"  Threads: 16 (parallel evaluation)\")\n",
    "print(\"\\nThe optimizer will:\")\n",
    "print(\"  1. Bootstrap few-shot examples from training data\")\n",
    "print(\"  2. Generate instruction candidates via LLM\")\n",
    "print(\"  3. Evaluate combinations on validation set\")\n",
    "print(\"  4. Select best performing configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c80cf",
   "metadata": {},
   "source": [
    "## Load Cache and Run Optimization\n",
    "\n",
    "â±ï¸ **Note**: We're using a pre-computed cache (`memory_cache.pkl`) to speed up the process for this demo.\n",
    "\n",
    "In production, you can run without cache - the optimizer will make real LLM calls.\n",
    "\n",
    "ðŸ”„ **Optimization Process** (typically 5-15 minutes without cache):\n",
    "- Tests multiple candidate programs\n",
    "- Continuously tracks and improves scores\n",
    "- Uses statistical sampling to find optimal configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Load Memory Cache\n",
    "dspy.cache.load_memory_cache(\"./memory_cache.pkl\")\n",
    "\n",
    "print(\"âœ“ Memory cache loaded from ./memory_cache.pkl\")\n",
    "print(\"  This speeds up the demo by avoiding redundant LLM calls\")\n",
    "print(\"  In production: Remove this line to run fresh optimization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Compile Optimized Program\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING OPTIMIZATION PROCESS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This will:\")\n",
    "print(\"  - Bootstrap few-shot examples\")\n",
    "print(\"  - Generate instruction candidates\")\n",
    "print(\"  - Evaluate candidate programs\")\n",
    "print(\"  - Track results in MLflow\")\n",
    "print(\"\\nðŸ”„ Compiling...\\n\")\n",
    "\n",
    "optimized_react = tp.compile(\n",
    "    react,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    "    requires_permission_to_run=False,  # Skip confirmation prompt\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ“ OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae997061",
   "metadata": {},
   "source": [
    "## Inspect Optimized Components\n",
    "\n",
    "Let's examine what the optimizer changed in our ReAct agent:\n",
    "\n",
    "### Original Program:\n",
    "- **Signature**: Simple `question -> answer` with no instructions\n",
    "- **Demos**: No few-shot examples\n",
    "\n",
    "### Optimized Program:\n",
    "- **Signature**: Enhanced with comprehensive instructions\n",
    "- **Demos**: Bootstrapped few-shot examples showing good reasoning traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: View Optimized Signature\n",
    "print(\"=\" * 70)\n",
    "print(\"ORIGINAL SIGNATURE:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"question -> answer\")\n",
    "print(\"(No instructions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZED SIGNATURE:\")\n",
    "print(\"=\" * 70)\n",
    "print(optimized_react.react.signature)\n",
    "print(\"\\nâœ“ Notice the comprehensive instructions added by the optimizer!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09014ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: View Optimized Demos\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZED FEW-SHOT EXAMPLES (Demos):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nNumber of demos: {len(optimized_react.react.demos)}\")\n",
    "\n",
    "if optimized_react.react.demos:\n",
    "    print(\"\\n--- First Demo ---\")\n",
    "    print(optimized_react.react.demos[0])\n",
    "    print(\"\\nâœ“ These are bootstrapped examples showing successful reasoning traces\")\n",
    "else:\n",
    "    print(\"No demos found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03779882",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "\n",
    "Now let's compare the **original** vs **optimized** agent performance on the validation set.\n",
    "\n",
    "**Metric**: Exact Match - The answer must match the ground truth exactly.\n",
    "\n",
    "We'll see how much improvement the optimizer achieved without any manual prompt engineering!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Setup Evaluator\n",
    "evaluator = dspy.Evaluate(\n",
    "    metric=dspy.evaluate.answer_exact_match,  # Exact match scoring\n",
    "    devset=valset,          # Evaluation dataset\n",
    "    display_table=True,     # Show results table\n",
    "    display_progress=True,  # Show progress bar\n",
    "    num_threads=24,         # Parallel evaluation\n",
    ")\n",
    "\n",
    "print(\"âœ“ Evaluator configured\")\n",
    "print(f\"  Metric: answer_exact_match\")\n",
    "print(f\"  Evaluation set: {len(valset)} examples\")\n",
    "print(f\"  Threads: 24\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Evaluate Original Agent\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING ORIGINAL REACT AGENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Running evaluation on validation set...\")\n",
    "print(\"(This shows baseline performance without optimization)\\n\")\n",
    "\n",
    "original_score = evaluator(react)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ðŸ“Š ORIGINAL SCORE: {original_score:.4f} ({original_score*100:.2f}%)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Evaluate Optimized Agent\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING OPTIMIZED REACT AGENT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Running evaluation on validation set...\")\n",
    "print(\"(This shows performance after MIPROv2 optimization)\\n\")\n",
    "\n",
    "optimized_score = evaluator(optimized_react)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ðŸ“Š OPTIMIZED SCORE: {optimized_score:.4f} ({optimized_score*100:.2f}%)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Performance Comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Metric':<20} {'Original':<15} {'Optimized':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "original_pct = original_score * 100\n",
    "optimized_pct = optimized_score * 100\n",
    "improvement = optimized_pct - original_pct\n",
    "relative_gain = (improvement / original_pct * 100) if original_pct > 0 else 0\n",
    "\n",
    "print(f\"{'Exact Match Score':<20} {original_pct:>6.2f}%{'':<8} {optimized_pct:>6.2f}%{'':<8} +{improvement:>5.2f}%\")\n",
    "print(f\"{'Relative Gain':<20} {'':<15} {'':<15} {relative_gain:>6.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ¨ ACHIEVEMENT UNLOCKED: Automatic Prompt Engineering!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWithout ANY manual prompt engineering, the MIPROv2 optimizer:\")\n",
    "print(f\"  âœ“ Generated optimized instructions automatically\")\n",
    "print(f\"  âœ“ Bootstrapped high-quality few-shot examples\")\n",
    "print(f\"  âœ“ Improved performance by {improvement:.2f} percentage points\")\n",
    "print(f\"  âœ“ Achieved {relative_gain:.1f}% relative improvement\")\n",
    "print(\"\\nThis is the power of DSPy Optimization!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
